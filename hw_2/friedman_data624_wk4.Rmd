---
title: "friedman_data624_hw2"
author: "Alice Friedman"
date: "2023-06-25"
output:
  html_document:
    theme: yeti
    highlight: tango
    toc: yes
    toc_float: yes
  pdf_document:
    dev: cairo_pdf
    toc: yes
---

```{r setup, include=FALSE, warning=FALSE}

knitr::opts_chunk$set(echo=TRUE, warning = F, message = F)
library(tidyverse)
library(kableExtra)
library(mice)
library(caret)
library(corrplot)
library(ggcorrplot)
library(MASS)  # for stepwise regression
library(forecast)

# Set minimal theme
theme_set(theme_minimal())

```

## Week 4: Linear Regression and Its Cousins
### KJ 6.3

**A chemical manufacturing process for a pharmaceutical product was discussed in Sect. 1.4. In this problem, the objective is to understand the relationship between biological measurements of the raw materials (predictors), measurements of the manufacturing process (predictors), and the response of the product yield. Biological predictors cannot be changed but can be used to assess the quality of the raw material before processing. On the other hand, manufacturing process predictors can be changed in the manufacturing process. Improving product yield by 1 % will boost revenue by approximately one hundred thousand dollars per batch.**

#### (a)

**Start R and use these commands to load the data:**

    > library(AppliedPredictiveModeling)
    > data(ChemicalManufacturingProcess)
    
**The matrix processPredictors contains the 57 predictors (12 describing the input biological material and 45 describing the process predictors) for the 176 manufacturing runs. yield contains the percent yield for each run.**

```{r KJ_6.3a}

# Load data
library(AppliedPredictiveModeling)
library(psych)
data(ChemicalManufacturingProcess)
head(ChemicalManufacturingProcess, 5)
describe(ChemicalManufacturingProcess)
```

#### (b)

**A small percentage of cells in the predictor set contain missing values. Use an imputation function to fill in those missing values (e.g., see Sect. 3.8).**

We'll first generate a summary to get a first look at the data and to show missing values, then we'll get a count of missing values per variable. We can impute the missing values using the `mice::complete` function. This function automatically imputes missing variables by running a multilinear regression using the values in other columns, using a strategy calledMultivariate Imputation By Chained Equations (MICE).

Reference: [MICE Algorithm to Impute Missing Values in a Dataset](https://www.numpyninja.com/post/mice-algorithm-to-impute-missing-values-in-a-dataset#:~:text=MICE%20stands%20for%20Multivariate%20Imputation,prediction%20for%20each%20missing%20value.)

```{r KJ_6.3b}

# Show missing value counts
dfchem_raw <- ChemicalManufacturingProcess  # To avoid typing this many letters
data.frame(Missing=colSums(is.na(dfchem_raw))) %>%
    filter(Missing > 0) %>%
    kbl(caption='Missing value counts') %>%
    kable_classic(full_width=F)

# Use MICE to impute missing values
imp <- mice(dfchem_raw, printFlag=F)
dfchem <- mice::complete(imp)

# Verify no more missing values exist
print(paste0('Missing values after imputation: ', sum(is.na(dfchem))))

```

### (c)

**Split the data into a training and a test set, pre-process the data, and tune a model of your choice from this chapter. What is the optimal value of the performance metric?**

First, we'll choose a random seed for reproducible results. Then we'll split the data using createDataPartition() from the `caret` package, with an 80/20 split for the training and test data.

```{r KJ_6.3c}
library(gridExtra)
# Set seed
set.seed(777)

# Split into train/test; createDataPartition generates indicies of the training set
train_indices <- createDataPartition(dfchem$Yield, p=0.80, times=1, list=F)
dftrain <- dfchem[train_indices,]
dftest <- dfchem[-train_indices,]


# Examine train and test sets
plot1 <- ggplot(data = dftrain, aes(x=Yield)) + geom_histogram(fill = "steelblue", binwidth = 1) + 
  ggtitle('Training set yield histogram') 

plot2 <- ggplot(data = dftest, aes(x=Yield)) + geom_histogram(fill = "steelblue", binwidth = 1) + 
  ggtitle('Test set yield histogram') 

grid.arrange(plot1, plot2, ncol = 2)


```

Now we'll examine the predictors to look for correlations and examine their relationship to the outcome variable.

```{r KJ_6.3c_correlation}
# Calculate the correlation matrix
corr_matrix <- cor(dftrain)

# Set threshold
cutoff <- 0.8

# Reshape, find candidates for removal
filtered_corr_long <- reshape2::melt(corr_matrix) %>% dplyr::filter(abs(value) > cutoff & value !=1)
filtered_corr <- reshape2::dcast(filtered_corr_long, Var1 ~ Var2)


# Plot the correlation matrix heatmap
ggplot(data = filtered_corr_long, aes(x = Var1, y = Var2, fill = value)) +
  geom_tile() +
  scale_fill_gradient(low = "white", high = "steelblue") +
  labs(title = "Correlation Matrix Heatmap Highly Correlated Variable") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

Because a large number of variables are highly correlated, a modeling technique which is robust to correlated variables is preferred, or else one feature of the highly correlated pair should be dropped. For this reason I will tune elastic net, which can handle multicollinearity.

```{r KJ_6.3c_model}
# Results data frame
dfr <- data.frame(matrix(nrow=1, ncol=4))
colnames(dfr) <- c('Model', 'Tuning.Parameters', 'Train.RMSE', 'Test.RMSE')

# Separate outcome and predictors
trainx <- dftrain %>% dplyr::select(-Yield)
trainy <- dftrain$Yield
testx <- dftest %>% dplyr::select(-Yield)
testy <- dftest$Yield

# specify 10x cross-validation
ctrl <- caret::trainControl(method='cv', number=10) # caret package

# elastic net regression
enetGrid <- expand.grid(.lambda=c(0, 0.01, 0.1), .fraction=seq(0.05, 1, length=20))
enet_fit <- train(x=trainx, y=trainy, method='enet', preprocess=c('center', 'scale'), trControl=ctrl, tuneGrid=enetGrid)
enet_fit %>% plot()

```


### (d)

**Predict the response for the test set. What is the value of the performance metric and how does this compare with the resampled performance metric on the training set?**

First, calculate the accuracy based on the test set.

```{r KJ_6.3d}

# Generate y predictions
predy <- predict(enet_fit, newdata=testx)

# Get accuracy results
enet_acc <- accuracy(predy, testy)

dfr[1,] = data.frame(
    Model='Elastic net', 
    Tuning.Parameters=
      paste0('lambda=', enet_fit$bestTune[['lambda']], ', fraction=', enet_fit$bestTune[['fraction']]), 
    Train.RMSE=enet_fit$results[['RMSE']] %>% round(2),
    Test.RMSE=enet_acc[[2]] %>% round(2) #RMSE for test set
)

dfr %>% kable(digits = 2)
```

The RMSE for the test set is very slightly higher than that of the training set (`r dfr[[4]]` vs `r dfr[[3]]`, respectively). This indicates good model fit and a lack of overfitting.

### (e)

**Which predictors are most important in the model you have trained? Do either the biological or process predictors dominate the list?**

Using the function `caret::varImp` we can interpret the results with a scaled list of the importance of each variables, where 100 is the  most important and 0 is the least. Of the top factors, about half are biological and half are manufacturing, while overall, manufacturing processes dominate.


```{r KJ_6.3e}
# reference http://topepo.github.io/caret/variable-importance.html
enetImp <- varImp(enet_fit, scale = T)
threhold <- 70
top <- enetImp$importance %>% subset(enetImp$importance$Overall > threhold)
top <- data.frame(Predictor = row.names(top), Scaled.Importance = top$Overall %>% round(0)) 
top %>% arrange(desc(Scaled.Importance))

```




### (f)

**Explore the relationships between each of the top predictors and the response. How could this information be helpful in improving yield in future runs of the manufacturing process?**

The approach is to subset the most important factors and then plot the data versus the yield. This could be heplful in identifying which processes to optimize to improve yield, especially in relationship to the cost associated with any changes.
```{r KJ_6.3f}
# Subset most important features
long <- dfchem[c(top$Predictor, "Yield")] %>% gather(key, value, -Yield)

ggplot(data = long, aes(x=value, y=Yield)) + geom_point() + geom_smooth(method = "lm") + facet_wrap(~key, scales = "free")

library(corrplot)
cor(dfchem[c(top$Predictor, "Yield")]) %>% corrplot()
```
* Manufacturing Process 32 has the most impact, and so would be a good place to start.
* All of the biological processes are highly correlated, which may or may mean that they can substituted for each other or that they are inherently related.
* Manufacturing Process 13 and 36 have a negative impact on Yield, while all the other top factors ahave a positive releationship withe Yield.

## Week 5
### 7.2

Friedman (1991) introduced several benchmark data sets create by simulation. One of these simulations used the following nonlinear equation to create data:

y = 10 sin(πx1x2) + 20(x3 − 0.5)2 + 10x4 + 5x5 + N(0, σ2)

where the x values are random variables uniformly distributed between [0, 1] (there are also 5 other non-informative variables also created in the simulation). The package mlbench contains a function called mlbench.friedman1 that simulates these data. Tune several models on these data. For example k-nearest neighbors. Which models appear to give the best performance? Does MARS select the informative predictors (those named X1–X5)?

*Approach*

We will test KNN, neural network, and MARS approaches. The data will be split 80/20 into training and test sets. Best performance will be determined by lowest MAPE on the test set.

```{r}
library(mlbench)
set.seed(200)
trainingData <- mlbench.friedman1(200, sd = 1)
trainingData$x <- data.frame(trainingData$x)
## Look at the data using
caret::featurePlot(trainingData$x, trainingData$y)
## or other methods.

## This creates a list with a vector 'y' and a matrix
## of predictors 'x'. Also simulate a large test set to
## estimate the true error rate with good precision:
testData <- mlbench.friedman1(5000, sd = 1)
testData$x <- data.frame(testData$x)

```


```{r knn}
library(caret)
knnModel <- train(x = trainingData$x, 
                    y = trainingData$y, 
                    method = "knn",
                    preProc = c("center", "scale"),
                    tuneLength = 10)

summary(knnModel)

knnPred <- predict(knnModel, newdata = testData$x)
## The function 'postResample' can be used to get the test set
## perforamnce values
postResample(pred = knnPred, obs = testData$y)

```
```{r nnet}
# From book
# Kuhn, Max; Johnson, Kjell. Applied Predictive Modeling (p. 162). Springer New York. Kindle Edition. 
set.seed(200)
nnetFit <- avNNet(trainingData$x, 
                  trainingData$y,  
                  size = 5,  
                  decay = 0.01,  
                  repeats = 5,
                  linout = TRUE,  
                  ##Reduce the amount of printed output  
                  trace = FALSE,  
                  ##Expand the number of iterations to find parameter estimates..  
                  maxit = 500,  
                  preProc = c("center", "scale"),
                  ##and the number of parameters used by the model  
                  MaxNWts = 5 * (ncol(trainingData$x) + 1) + 5 + 1
                  )  

summary(nnetFit)

nnetPred <- predict(nnetFit, newdata = testData$x)
postResample(pred = nnetPred, obs = testData$y)



```

```{r tuned_nnet}
#from book: Kuhn, Max; Johnson, Kjell. Applied Predictive Modeling (p. 163). Springer New York. Kindle Edition. 

 ##The findCorrelation takes a correlation matrix and determines thee column numbers that should be removed to keep all pair-wise correlations below a threshold  
tooHigh <- findCorrelation(cor(trainingData$x), 
                           cutoff = .75)  
trainXnnet <- trainingData$x[, -tooHigh]  
testXnnet <- testData$x[, -tooHigh]  
##Create a specific candidate set of models to evaluate:  
nnetGrid <- expand.grid(.decay = c(0, 0.01, .1),  .size = c(1:10),  
                        ##The next option is to use bagginginstead of different random  + ##seeds.  
                        .bag = FALSE)  
set.seed(100)  
nnetTunedFit <- train(trainXnnet, 
                  trainingData$y,
                  method = "avNNet",  
                  tuneGrid = nnetGrid,  
                  trControl = ctrl,  #what is this doing?
                  ##Automatically standardize data prior to modeling  and prediction  
                  preProc = c("center", "scale"),  
                  linout = TRUE,  
                  trace = FALSE,  
                  MaxNWts = 10 * (ncol(trainXnnet) + 1) + 10 + 1,  
                  maxit = 500)  

nnet_tunedPred <- predict(nnetTunedFit, newdata = testXnnet)
postResample(pred = nnet_tunedPred, obs = testData$y)
```

```{r mars}
library(earth)
marsFit <- earth(trainingData$x, trainingData$y)
marsFit

marsPred <- predict(marsFit, newdata = testData$x)
postResample(pred = marsPred, obs = testData$y)

```

**Interpretation of Results**
The lowest MAPE results on the test set is from the neural net, which is ~1.15.


### 7.5

Exercise 6.3 describes data for a chemical manufacturing process. Use the same data imputation, data splitting, and pre-processing steps as before and train several nonlinear regression models.

(a) Which nonlinear regression model gives the optimal resampling and test set performance?

**Approach**

First, set up a data frame for the results and set the cross-validation parameters. Then, preprocess the data using MICE to impute empty values. Finally, split the data and run KNN, neural network, lasso, and SVM. See what's the best results based on lowest MAPE comparing predictors to test set.

To simplify what are several repeated steps, we will create a function that takes as inputs a dataframe, model to test ("func), and a list of parameters to call with that funciton.

```{r 7.5}

dfchem_raw <- ChemicalManufacturingProcess  # To avoid typing this many letters

prep <- function(df){

  # Impute missing values
  imp <- knnImputation(df, k=3)
  set.seed(77)
  df_complete <- complete(imp)

  # Remove the highly correlated variables
  high_corr <- findCorrelation(cor(df_complete), cutoff=0.9, exact=T, verbose=F, names=F)
  df_remove_corr <- df_complete[,-high_corr]

  # Remove NZV features
  tmp_nzv <- nearZeroVar(df_remove_corr)
  df_clean <- df_remove_corr[,-tmp_nzv]
  
  return(df_clean)
  
}

dfchem <- prep(dfchem_raw)

# specify 10x cross-validation
ctrl <- trainControl(method='cv', number=10)

# Set seed
set.seed(77)

# Split into train/test; createDataPartition generates indicies of the training set
train_indices <- createDataPartition(dfchem$Yield, p=0.80, times=1, list=F)
dftrain <- dfchem[train_indices,]
dftest <- dfchem[-train_indices,]

#test set
testy <- dftest$Yield
testx <- dftest %>% dplyr::select(-Yield)

#train set
trainx <- dftrain %>% dplyr::select(-Yield)
trainy <- dftrain$Yield

# define function to train, predict, and evaluate model fit
modelTest <- function(params, testx, testy, func){ 
  # "func" allows you to pass function as a parameter
  
  print(
    paste(
      "Running model",
      params$method
    ))
  
  set.seed(77)  
  fit <- do.call(func, params)
  
  pred <- predict(fit, newdata = testx)
  
  res <- postResample(pred = pred, obs = testy)
  
  print(res)

  return(res)

}


# create params for models to test 
## SVM model
svmParams <- list(
  x=trainx, y=trainy, method='svmRadial', preProc=c('center', 'scale'), tuneLength=14, trControl=ctrl
  )
## KNN model
knnParams <- list(
  x=trainx, y=trainy, method='knn', tuneLength=10, trControl=ctrl, preProc=c('center', 'scale')
  )
## MARS model
MARSParams <- list(
  x=trainx, y=trainy, method='earth', 
  tuneGrid=expand.grid(.degree=1:2, .nprune=2:10),  # set tuning parameters
  trControl=ctrl
  )
  
## nnet model using model averaging
nnetParams <- list(
  x=trainx, y=trainy, method='avNNet', preProc=c('center', 'scale'), 
  tunGrid=expand.grid(.decay=c(0, 0.01, 0.1), .size=c(1:10), .bag=c(T, F)),  # set tuning parameters 
  trControl=ctrl,
  linout=T, 
  trace=F,
  MaxNWts=10 * (ncol(trainx) + 1) + 10 + 1, 
  maxit=500
)

# create list of params
params <- list(
  svmParams,
  knnParams,
  MARSParams,
  nnetParams
  )

# Results data frame
results <- data.frame()

for (param in params){
  res <- modelTest(param, testx, testy, train)
  row <- data.frame(Model = param$method, 
                    RMSE = res[['RMSE']])
  results <- rbind(results, row)
}

colnames(results) <- c('Model', 'RMSE')

results %>% dplyr::arrange(RMSE) %>% flextable::flextable()
```

```{r}
results %>% ggplot(aes(x=Model, y = RMSE, fill = Model)) +geom_col(stat="identity")
```

**Interpretation of Results**

The lowest RMSE when tested against the test set is from the 'earth' method, which is running the MARS model type; however none of the models produce very dramatically different results.

(b) Which predictors are most important in the optimal nonlinear regression model? Do either the biological or process variables dominate the list? How do the top ten important predictors compare to the top ten predictors from the optimal linear model?

**Approach**

We will use the function `varImp` from the package `caret` to calculate the relative important of each variable.
```{r}
earthFit <- do.call(train, MARSParams)
varImp(earthFit)
```

**Interpretation of Results**
(c) Explore the relationships between the top predictors and the response for the predictors that are unique to the optimal nonlinear regression model. Do these plots reveal intuition about the biological or process predictors and their relationship with yield?

**Approach**
We can calculate the correlation between the top predictor and the yield, and also calculate the correlations. 
```{r}
plot(dfchem$ManufacturingProcess32, dfchem$Yield)
cor(dfchem$ManufacturingProcess32, dfchem$Yield)
```

**Interpretation of Results**
We can see a fairly strong linear relationship between the predictor and `Yield`. Although MARS is a non-linear approach, it is made up of multiple linear regressions so it is not surprising that the top predictor shows a strong linear relationship with the target.

### 8.1

Recreate the simulated data from Exercise 7.2

```{r}
library(mlbench)
set.seed(200)
simulated <- mlbench.friedman1(200, sd = 1)
simulated <- cbind(simulated$x, simulated$y)
simulated <- as.data.frame(simulated)
colnames(simulated)[ncol(simulated)] <- "y"
```

(a) Fit a random forest model to all of the predictors, then estimate the variable importance scores. Did the random forest model significantly use the uninformative predictors (V6 – V10)?


**Approach**

**Code**
```{r}
library(randomForest)
library(caret)
model1 <- randomForest(y ~ ., data = simulated,
                       importance = TRUE,
                       ntree = 1000)
rfImp1 <- varImp(model1, scale = FALSE)
rfImp1
```
**Interpretation of Results**



(b) Now add an additional predictor that is highly correlated with one of the informative predictors. For example, fit another random forest model to these data. Did the importance score for V1 change? What happens when you add another predictor that is also highly correlated with V1?

**Approach**
**Code**

```{r}
simulated$duplicate1 <- simulated$V1 + rnorm(200) * .1
cor(simulated$duplicate1, simulated$V1)
```

**Interpretation of Results**




(c) Use the cforest function in the party package to fit a random forest model using conditional inference trees. The party package function varimp can calculate predictor importance. The conditional argument of that function toggles between the traditional importance measure and the modified version described in Strobl et al. (2007). Do these importances show the same pattern as the traditional random forest model?

**Approach**
**Code**

```{r}

```

**Interpretation of Results**

(d) Repeat this process with different tree models, such as boosted trees and Cubist. Does the same pattern occur?

**Approach**
**Code**

```{r}

```

**Interpretation of Results**


### 8.2

Use a simulation to show tree bias with different granularities.

**Approach**
**Code**

```{r}

```

**Interpretation of Results**


### 8.3

In stochastic gradient boosting the bagging fraction and learning rate will govern the construction of the trees as they are guided by the gradient. Although the optimal values of these parameters should be obtained through the tuning process, it is helpful to understand how the magnitudes of these parameters affect magnitudes of variable importance. Figure 8.24 provides the variable importance plots for boosting using two extreme values for the bagging fraction (0.1 and 0.9) and the learning rate (0.1 and 0.9) for the solubility data. The left-hand plot has both parameters set to 0.1, and the right-hand plot has both set to 0.9:

(a) Why does the model on the right focus its importance on just the first few of predictors, whereas the model on the left spreads importance across more predictors?

**Approach**
**Code**

```{r}

```

**Interpretation of Results**


(b) Which model do you think would be more predictive of other samples?

**Approach**
**Code**

```{r}

```

**Interpretation of Results**


(c) How would increasing interaction depth affect the slope of predictor importance for either model in Fig. 8.24?

**Approach**
**Code**

```{r}

```

**Interpretation of Results**


### 8.7

Refer to Exercises 6.3 and 7.5 which describe a chemical manufacturing process. Use the same data imputation, data splitting, and pre-processing steps as before and train several tree-based models:

(a) Which tree-based regression model gives the optimal resampling and test
set performance?
**Approach**
**Code**

```{r}

```

**Interpretation of Results**

(b) Which predictors are most important in the optimal tree-based regression model? Do either the biological or process variables dominate the list? How do the top 10 important predictors compare to the top 10 predictors from the optimal linear and nonlinear models?

**Approach**
**Code**

```{r}

```

**Interpretation of Results**

(c) Plot the optimal single tree with the distribution of yield in the terminal nodes. Does this view of the data provide additional knowledge about the biological or process predictors and their relationship with yield?

**Approach**
**Code**

```{r}

```

**Interpretation of Results**


### Recommender Problem


