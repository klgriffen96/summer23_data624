---
title: "ippolito_data624_proj2"
author: "Michael Ippolito"
date: "2023-07-04"
output: html_document
---

output:
  html_document:
    theme: yeti
    highlight: tango
    toc: yes
    toc_float: yes
  pdf_document:
    dev: cairo_pdf
    toc: yes
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(echo=TRUE, fig.width=9, fig.height=6)
library(tidyverse)
library(caret)
library(corrplot)
library(mice)
library(e1071)

# Set minimal theme
theme_set(theme_minimal())

```

### Load data

First, we load the data from Github. We had some trouble reading the Excel files, so we converted them to CSV.

```{r}

# Load training data (m=modeling)
dfm_raw <- read.csv('https://raw.githubusercontent.com/klgriffen96/summer23_data624/main/project_2/StudentData%20-%20TO%20MODEL.csv')

# Load evaluation data
dfe_raw <- read.csv('https://github.com/klgriffen96/summer23_data624/raw/main/project_2/StudentEvaluation-%20TO%20PREDICT.csv')

```

### Exploratory data analysis

#### Preliminary view

A first look at the data is as follows.

```{r}

# Move outcome variable pH to front for easier access
dfm <- dfm_raw %>%
    dplyr::select(PH, !matches('Brand.Code'), Brand.Code)
str(dfm)
summary(dfm)

```

We note several things from a cursory first glance:

1) There are 2571 observations in 33 variables.
1) The outcome variable, pH, is continuous, so this is a regression problem.
1) Of the 32 predictors, 31 are quantitative variables and a single nominal categorical variable, Brand.Code.
1) There are a number of missing values but not at a rate that would be debilitating to fitting a model to the data. Additional investigation will be done to determine how best to handle these data points.
1) Little information was given about the data, so we can't infer units for the variables (for example, it is unknown whether the temperature and pressure variables are in English or metric units).

We'll need to do a small amount of data wrangling first: The categorical variable Brand.Code will need to be factored into numerical levels, as some functions that we'll use later requires it (for example near-zero variance calculations, seen later).

```{r}

# Factor categorical variable Brand.Code
dfm$Brand.Code <- factor(dfm$Brand.Code)

```

#### Feature plots

Next, we'll generate feature plots of the continuous variables and a boxplot of the single categorical variable (Brand.Code). These plot show the relationship of each continuous variable against the outcome (pH) and can be used to detect how individual predictors influence the outcome.

```{r fig.width=14, fig.heigh=12, warning=F}

# Feature plots - exclude the last column, which is Brand.Code (a categorical variable)
featurePlot(x=dfm[,2:(ncol(dfm)-1)], y=dfm$PH, plot='scatter', main='Feature plots against PH', type=c('p', 'smooth'), span=0.5, layout=c(8, 4), col.line='red')

# Boxplot of brand against PH
dfm %>%
    filter(!is.na(Brand.Code) & !is.na(PH)) %>%
    ggplot(aes(x=Brand.Code, y=PH)) +
    geom_boxplot()

```

At first glance there doesn't appear to be any strong relationship between pH and any of the predictors. There is a slightly positive relationship between pH and some variables like pressure vacuum, but in general no strong trends are evident.

#### Collinearity

We'll also look for collinearity among variables. Collinearity is a problem for some many types of models that rely on a linear algebraic solution to generate coefficients, as these models can generate severely inflated coefficients for predictors that are strongly collinear. In these cases, we have several options:

1) Retain only a single predictor among those that are collinear relative to each other.
1) Transform the predictors such that collinearity is minimized, for example using principle component analysis (PCA), partial least squares (PLS), or linear discriminant analysis (LDA).
1) Employ a model that is insensitive (or less sensitive) to collinear predictors, for example ridge, lasso, PLS, or multivariate adaptive regression splines (MARS).

We'll use the corrplot() function from the package of the same name to generate a correlation plot. This shows strongly correlated variables in darker colors and with larger squares. Positive correlations are shown in blue, while negative correlations are in red. Correlations of 1.0 indicate perfect positive correlation between variables, while correlations of -1.0 indicate a perfect negative correlation. The plot is clustered by variables that exhibit strong correlations with each other, which can provide some indication of multicollinearity among groups of variables.

```{r fig.width=11, fig.heigh=8}

# Generate corr plot for pairwise correlations
corr1 <- cor(dfm[,1:(ncol(dfm)-1)], use='complete')
corrplot::corrplot(corr1, method='square', order='hclust', type='full', diag=T, tl.cex=0.75, cl.cex=0.75)

```

As shown in the correlation plot, there are some strong correlations among the following six variables:

- Carb.Volume
- Carb.Rel
- Alch.Rel
- Density
- Bailing
- Bailing.Lvl

It may be advantageous to remove one or more of these variables, transform them to remove collinearity, or choose a model that is insensitive to collinearity. Interestingly, these same six variables exhibit a relatively strong inverse relationship with Hyd.Pressure4.

#### Multicollinearity

In addition to collinearity, which evaluates relationships between pairs of predictors, we'll look at multicollinearity, which gives a general indication of how much each predictor is related to the other predictors than to the response variable. To evaluate multicollinearity, we'll fit a simple linear model to the data using the full set of predictors. Then, we'll use the vif() function in the cars package to estimate the variance inflation factor (VIF), which is a statistic purpose-built to measure collinearity among variables. Values less than or equal to 1 indicate no multicollinearity, values between 1 and 5 indicate moderate multicollinearity, and values greater than 5 indicate strong multicollinearity (Glen, 2023).

```{r}

# Examine multicollinearity by generating a simple linear model of the data,
# then looking at the variance inflation factor (VIF) of each variable.
#     VIF <= 1:      low multicollinearity
#     1 < VIF <= 5:  moderate multicollinearity
#     VIF > 5:       high multicollinearity
library(car)
lmod <- lm(PH ~ ., data=dfm)
dfvif <- data.frame(vif(lmod)) %>%
    mutate(Predictor=row.names(dfvif)) %>%
    rename(VIF=GVIF) %>%
    dplyr::select(Predictor, VIF) %>%
    arrange(desc(VIF)) %>%
    mutate(Rank=rank(-VIF))
dfvif %>%
    flextable() %>%
    width(width = 2) %>%
    fontsize(size = 10) %>%
    line_spacing(space = 1) %>%
    hline(part = "all") %>%
    set_caption('Variance Inflation Factor')

```

As shown in the table almost half (15) of the variables have a VIF greater than 5, which indicate that they exhibit strong multicollinearity. This will inform our decision on the type of model to employ.

#### Near-zero-variance predictors

We'll look for variables having near-zero variance (NZV) since some models are sensitive to this phenomenon. A variable exhibiting NZV is typically categorical and is further characterized by one with only a single value (or a very small number of values) across the entire range of observations. These types of "degenerate" distributions can be problematic for models such as those based on linear regression. If NZV variables are observed, either they should be removed or a model that is immune to NZV variables should be used (e.g. tree-based models).

To calculate NZV, we'll use the NearZeroVar() function from the caret package. This calculates the ratio of the frequency of the most common value in each variable to that of the next most common value. If the ratio is high, this indicates NZV conditions, and the variable may need special handling if a model that is sensitive to NZV variables are being used.

The following table shows the frequency ratios of variables in the data set in descending order.

```{r}

# Look for NZV variables
nzv <- nearZeroVar(dfm, saveMetrics=T)
nzv$Variable <- row.names(nzv)
nzv %>% dplyr::select(Variable, everything()) %>%
    arrange(desc(freqRatio)) %>%
    flextable() %>%
    width(width = 2) %>%
    fontsize(size = 10) %>%
    line_spacing(space = 1) %>%
    hline(part = "all") %>%
    set_caption('Frequency ratios of variables')

```

As shown, only a single variable (Hyd.Pressure1) exhibited a high enough frequency ratio to be classified as an NZV variable. We'll examine this variable more.

```{r}

# Generate table of top unique values of Hyd.Pressure1
hpfreq <- dfm %>%
    group_by(Hyd.Pressure1) %>%
    summarize(Unique.count=n()) %>%
    arrange(desc(Unique.count))

# Tally up values other than the top 2
others <- hpfreq %>%
    filter(Unique.count < hpfreq[[2,'Unique.count']]) %>%
    summarize(Unique.count=sum(Unique.count))

# Add the "other values" row to the table
hpfreq <- hpfreq %>%
    head(2) %>%
    rbind(data.frame(Hyd.Pressure1='other values', Unique.count=as.numeric(others))) %>%
    rename(Value=Hyd.Pressure1) %>%
    mutate(Percent=round(100 * Unique.count / nrow(dfm), 2))

# Display the table
hpfreq %>%
    flextable() %>%
    width(width = 2) %>%
    fontsize(size = 10) %>%
    line_spacing(space = 1) %>%
    hline(part = "all") %>%
    set_caption('Hyd.Pressure1')

```

As indicated in the table above, a large proportion of the observations have zero values (32.7%). Without knowing anything about how the data was collected or what the variable indicates, it is difficult to know the best way to handle this variable. The best we can do is make an assumption that zero values are structurally sound, correctly entered, and have meaning in the context of the variable. Based on these assumptions, this variable is a candidate to be dropped if a linear regression-based model is used.

#### Missing values

Now we'll examine missing values in the data set. As noted earlier, there aren't that many, but some models are sensitive to missing values. As such, the following options are avaiable:

1) Discard observations with missing values for many of the predictors.
1) Discard entire predictors that have many values missing across a great number of observations.
1) Impute (fill in) the missing values using any of a number of statistical methods available (e.g. mean, median, or K nearest neighbor modeling).

We'll take a first look at missing values on a column-by-column basis.

```{r}

# Count NAs per column
missing_values <- data.frame(colSums(is.na(dfm)))
missing_values$Variable <- row.names(missing_values)
colnames(missing_values) <- c('Missing.values', 'Variable')
missing_values %>%
    dplyr::select(Variable, Missing.values) %>%
    arrange(desc(Missing.values)) %>%
    flextable() %>%
    width(width = 2) %>%
    fontsize(size = 10) %>%
    line_spacing(space = 1) %>%
    hline(part = "all") %>%
    set_caption('Missing values')

```

A good way to visualize missing values is to use the md.pattern() function from the mice package. This function produces a plot that shows patterns of missing data in order of increasing missing information. The numbers along the left indicate the number of observations that correspond to the pattern indicated by that row of squares. Blue squares indicate no missing values, while red squares indicate missing values. The numbers on the far right indicate the number of variables with missing data for that set of observations.

```{r fig.height=16, fig.width=20}

# Missing value patterns
md.pattern(dfm, rotate.names=T, plot=T)

```

As shown, there are 2129 observations for which no values are missing, or 82.8% of the data. We note that the number of rows with many red squares is small, indicating that there aren't many observations missing a significant number of variables. We only saw a single observation that had missing values numbering in the double digits of predictors (12); this row is likely a good candidate for removal. The remaining data can likely be imputed (filled in) using one of the statistical methods listed above. An additional option is to use multivariate imputation by chained equations (MICE) (van Buren et al., 2023). MICE uses the fully conditional specification (FCS), which imputes each variable using a separate method. The method used can either be specified by the modeler or chosen by a set of defaults, depending on the kind of variable, for example, continuous or categorical, with additional methods for categorical variables that depend on the number of factors.

#### Skewness

Many types of models are sensitive to variables that exhibit skewed distributions, i.e. those that do not exhibit a typical bell-shaped pattern but instead either have a great many high values or have a great many low values. Linear-based models are particularly susceptible to skewness. Skewed variables can often be mathematically transformed to reduce skewness, for example taking the natural logarithm or using a method such as Box-Cox. The Box-Cox method uses an empirical means to find a transformation of the data.

Box Cox transformations only work on positive values, so if zero or negative values are encountered a different method must be used. If only zero values are observed, a constant (typically 1) can be added to the value before applying the Box-Cox transformation; however, this is debate on what constant to use in these cases. Alternatively, if both zero and negative values are present, a method such as one proposed by Yeo and Johnson (Yeo & Johnson, 2000) can be employed to transform the variable. The Yeo-Johnson method can accept positive, negative, or zero values and is thus more robust.

To visually examine skewness, we'll generate histograms of the continuous variables.

```{r fig.width=12, fig.height=10, warning=F}

# Generate historgrams of continuous variables
dfm %>%
    dplyr::select(c(1:(ncol(dfm)-1))) %>%
    gather() %>%
    ggplot(aes(x=value)) +
    geom_histogram(bins=20) +
    facet_wrap(~key, scales = 'free_x') +
    ggtitle('Histograms of continuous variables')


```

Some variables exhibit obviously skewed distributions (e.g. Filler.Speed, PSC, and Usage.cont), while others appear to be normally distributed (i.e., approximately symmetric), for example PH and PSC.Fill.

To gain a better understanding of which variables are skewed and to what degree they are skewed, we can calculate skewness quantitatively using the e1071 package (Joanes and Gill, 1998). Skewness with absolute values that are greater than 1 are considered highly skewed, while those between 0.5 and 1 are moderately skewed. Absolute values less than 0.5 can be considered approximately symmetric.

The sign of the skewness value indicates in which direction the distribution is skewed: Left-skewed distributions will exhibit negative skewness values, while the sign of right-skewed distributions will be positive.

The following table quantitatively enumerates the skewness of each variable.

```{r}

# Look at skewness quantitatively
# As a general rule of thumb:
#     highly skewed:            |skewness| > 1
#     moderately skewed:        0.5 < |skewness| < 1
#     approximately symmetric:  0 < |skewness| < 0.5
# Negative values indicate left-skewed distributions; positive values indicate right-skewed distributions.

# Create function to calculate skewness (needed because it handles columns with missing values inconsistently)
calcSkewness <- function(x, type) {
    new_x <- x
    if (sum(is.na(x)) > 0) {
        new_x <- !is.na(new_x)
    }
    return(e1071::skewness(new_x, type=type))
}

# Calculate skewness on columns
colskewness <- data.frame(apply(dfm[,1:(ncol(dfm)-1)], 2, calcSkewness, type=1))
colskewness$Variable <- row.names(colskewness)
colnames(colskewness) <- c('Skewness', 'Variable')
colskewness %>%
    dplyr::select(Variable, Skewness) %>%
    arrange(desc(abs(Skewness))) %>%
    flextable() %>%
    width(width = 2) %>%
    fontsize(size = 10) %>%
    line_spacing(space = 1) %>%
    hline(part = "all") %>%
    set_caption('Variable skewness')

```

As seen in the table, every single variable is heavily skewed with the exception of Pressure.Vacuum. And excpet for Air.Pressure, all of the skewed variables are skewed to the left (Air.Pressure is right-skewed). This reinforces the idea that some type of transformation will be needed if a model is used that is sensitive to skewness (e.g. linear regression-based models). Alternatively, models that aren't sensitive to skewness can be employed instead (for example tree-based models).

#### Outliers



```{r}



```


```{r}



```


```{r}



```


```{r}



```


```{r}



```


```{r}



```


```{r}



```


```{r}



```


```{r}



```


```{r}



```


```{r}



```


```{r}



```


```{r}



```


```{r}



```


```{r}



```


```{r}



```


```{r}



```

### References

Kuhn, M. and Johnson, Kjell (2013). Applied Predictive Modeling, Springer Science+Bsuiness Media.

Stephanie Glen (2023). "Variance Inflation Factor" From StatisticsHowTo.com: Elementary Statistics for the rest of us! https://www.statisticshowto.com/variance-inflation-factor/.

https://www.rdocumentation.org/packages/regclass/versions/1.6/topics/VIF

https://www.rdocumentation.org/packages/mice/versions/3.16.0/topics/md.pattern

van Buren, S., et al. (2023). Multivariate Imputation by Chained Equations. https://cran.r-project.org/web/packages/mice/mice.pdf.

van Buren, S. https://stefvanbuuren.name/fimd/sec-FCS.html

Box, G. and Cox, D. (1964). An Analysis of Transformations. Journal of the Royal Statistical Society: Series B (Methodological), Volume 26, Issue 2, June 1964, Pages 211-243. https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/j.2517-6161.1964.tb00553.x

Yeo, I. and Johnson, R. (2000). A new family of power transformations to improve normality or symmetry. Biometrika, Volume 87, Issue 4, December 2000, Pages 954–959. https://academic.oup.com/biomet/article-abstract/87/4/954/232908.

D. N. Joanes and C. A. Gill (1998), Comparing measures of sample skewness and kurtosis. The Statistician, 47, 183--189.
