---
title: "friedman_data624_wk4"
author: "Alice Friedman"
date: "2023-06-25"
output:
  html_document:
    theme: yeti
    highlight: tango
    toc: yes
    toc_float: yes
  pdf_document:
    dev: cairo_pdf
    toc: yes
---

```{r setup, include=FALSE, warning=FALSE}

knitr::opts_chunk$set(echo=TRUE, warning = F, message = F)
library(tidyverse)
library(kableExtra)
library(mice)
library(caret)
library(corrplot)
library(ggcorrplot)
library(MASS)  # for stepwise regression
library(forecast)

# Set minimal theme
theme_set(theme_minimal())

```

## Week 4: Linear Regression and Its Cousins
### KJ 6.3

**A chemical manufacturing process for a pharmaceutical product was discussed in Sect. 1.4. In this problem, the objective is to understand the relationship between biological measurements of the raw materials (predictors), measurements of the manufacturing process (predictors), and the response of the product yield. Biological predictors cannot be changed but can be used to assess the quality of the raw material before processing. On the other hand, manufacturing process predictors can be changed in the manufacturing process. Improving product yield by 1 % will boost revenue by approximately one hundred thousand dollars per batch.**

#### (a)

**Start R and use these commands to load the data:**

    > library(AppliedPredictiveModeling)
    > data(ChemicalManufacturingProcess)
    
**The matrix processPredictors contains the 57 predictors (12 describing the input biological material and 45 describing the process predictors) for the 176 manufacturing runs. yield contains the percent yield for each run.**

```{r KJ_6.3a}

# Load data
library(AppliedPredictiveModeling)
library(psych)
data(ChemicalManufacturingProcess)
head(ChemicalManufacturingProcess, 5)
describe(ChemicalManufacturingProcess)
```

#### (b)

**A small percentage of cells in the predictor set contain missing values. Use an imputation function to fill in those missing values (e.g., see Sect. 3.8).**

We'll first generate a summary to get a first look at the data and to show missing values, then we'll get a count of missing values per variable. We can impute the missing values using the `mice::complete` function. This function automatically imputes missing variables by running a multilinear regression using the values in other columns, using a strategy calledMultivariate Imputation By Chained Equations (MICE).

Reference: [MICE Algorithm to Impute Missing Values in a Dataset](https://www.numpyninja.com/post/mice-algorithm-to-impute-missing-values-in-a-dataset#:~:text=MICE%20stands%20for%20Multivariate%20Imputation,prediction%20for%20each%20missing%20value.)

```{r KJ_6.3b}

# Show missing value counts
dfchem_raw <- ChemicalManufacturingProcess  # To avoid typing this many letters
data.frame(Missing=colSums(is.na(dfchem_raw))) %>%
    filter(Missing > 0) %>%
    kbl(caption='Missing value counts') %>%
    kable_classic(full_width=F)

# Use MICE to impute missing values
imp <- mice(dfchem_raw, printFlag=F)
dfchem <- mice::complete(imp)

# Verify no more missing values exist
print(paste0('Missing values after imputation: ', sum(is.na(dfchem))))

```

### (c)

**Split the data into a training and a test set, pre-process the data, and tune a model of your choice from this chapter. What is the optimal value of the performance metric?**

First, we'll choose a random seed for reproducible results. Then we'll split the data using createDataPartition() from the `caret` package, with an 80/20 split for the training and test data.

```{r KJ_6.3c}
library(gridExtra)
# Set seed
set.seed(777)

# Split into train/test; createDataPartition generates indicies of the training set
train_indices <- createDataPartition(dfchem$Yield, p=0.80, times=1, list=F)
dftrain <- dfchem[train_indices,]
dftest <- dfchem[-train_indices,]


# Examine train and test sets
plot1 <- ggplot(data = dftrain, aes(x=Yield)) + geom_histogram(fill = "steelblue", binwidth = 1) + 
  ggtitle('Training set yield histogram') 

plot2 <- ggplot(data = dftest, aes(x=Yield)) + geom_histogram(fill = "steelblue", binwidth = 1) + 
  ggtitle('Test set yield histogram') 

grid.arrange(plot1, plot2, ncol = 2)


```

Now we'll examine the predictors to look for correlations and examine their relationship to the outcome variable.

```{r KJ_6.3c_correlation}
# Calculate the correlation matrix
corr_matrix <- cor(dftrain)

# Set threshold
cutoff <- 0.8

# Reshape, find candidates for removal
filtered_corr_long <- reshape2::melt(corr_matrix) %>% dplyr::filter(abs(value) > cutoff & value !=1)
filtered_corr <- reshape2::dcast(filtered_corr_long, Var1 ~ Var2)


# Plot the correlation matrix heatmap
ggplot(data = filtered_corr_long, aes(x = Var1, y = Var2, fill = value)) +
  geom_tile() +
  scale_fill_gradient(low = "white", high = "steelblue") +
  labs(title = "Correlation Matrix Heatmap Highly Correlated Variable") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

```

Because a large number of variables are highly correlated, a modeling technique which is robust to correlated variables is preferred, or else one feature of the highly correlated pair should be dropped. For this reason I will tune elastic net, which can handle multicollinearity.

```{r KJ_6.3c_model}
# Results data frame
dfr <- data.frame(matrix(nrow=1, ncol=4))
colnames(dfr) <- c('Model', 'Tuning.Parameters', 'Train.RMSE', 'Test.RMSE')

# Separate outcome and predictors
trainx <- dftrain %>% dplyr::select(-Yield)
trainy <- dftrain$Yield
testx <- dftest %>% dplyr::select(-Yield)
testy <- dftest$Yield

# specify 10x cross-validation
ctrl <- caret::trainControl(method='cv', number=10) # caret package

# elastic net regression
enetGrid <- expand.grid(.lambda=c(0, 0.01, 0.1), .fraction=seq(0.05, 1, length=20))
enet_fit <- train(x=trainx, y=trainy, method='enet', preprocess=c('center', 'scale'), trControl=ctrl, tuneGrid=enetGrid)
enet_fit %>% plot()

```


### (d)

**Predict the response for the test set. What is the value of the performance metric and how does this compare with the resampled performance metric on the training set?**

First, calculate the accuracy based on the test set.

```{r KJ_6.3d}

# Generate y predictions
predy <- predict(enet_fit, newdata=testx)

# Get accuracy results
enet_acc <- accuracy(predy, testy)

dfr[1,] = data.frame(
    Model='Elastic net', 
    Tuning.Parameters=
      paste0('lambda=', enet_fit$bestTune[['lambda']], ', fraction=', enet_fit$bestTune[['fraction']]), 
    Train.RMSE=enet_fit$results[['RMSE']] %>% round(2),
    Test.RMSE=enet_acc[[2]] %>% round(2) #RMSE for test set
)

dfr %>% kable(digits = 2)
```

The RMSE for the test set is very slightly higher than that of the training set (`r dfr[[4]]` vs `r dfr[[3]]`, respectively). This indicates good model fit and a lack of overfitting.

### (e)

**Which predictors are most important in the model you have trained? Do either the biological or process predictors dominate the list?**

Using the function `caret::varImp` we can interpret the results with a scaled list of the importance of each variables, where 100 is the  most important and 0 is the least. Of the top factors, about half are biological and half are manufacturing, while overall, manufacturing processes dominate.


```{r KJ_6.3e}
# reference http://topepo.github.io/caret/variable-importance.html
enetImp <- varImp(enet_fit, scale = T)
threhold <- 70
top <- enetImp$importance %>% subset(enetImp$importance$Overall > threhold)
top <- data.frame(Predictor = row.names(top), Scaled.Importance = top$Overall %>% round(0)) 
top %>% arrange(desc(Scaled.Importance))

```




### (f)

**Explore the relationships between each of the top predictors and the response. How could this information be helpful in improving yield in future runs of the manufacturing process?**

The approach is to subset the most important factors and then plot the data versus the yield. This could be heplful in identifying which processes to optimize to improve yield, especially in relationship to the cost associated with any changes.
```{r KJ_6.3f}
# Subset most important features
long <- dfchem[c(top$Predictor, "Yield")] %>% gather(key, value, -Yield)

ggplot(data = long, aes(x=value, y=Yield)) + geom_point() + geom_smooth(method = "lm") + facet_wrap(~key, scales = "free")

library(corrplot)
cor(dfchem[c(top$Predictor, "Yield")]) %>% corrplot()
```
* Manufacturing Process 32 has the most impact, and so would be a good place to start.
* All of the biological processes are highly correlated, which may or may mean that they can substituted for each other or that they are inherently related.
* Manufacturing Process 13 and 36 have a negative impact on Yield, while all the other top factors ahave a positive releationship withe Yield.

