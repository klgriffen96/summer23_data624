---
title: "DATA 624 HW4"
author: "Josh Iden"
date: "`r Sys.Date()`"
output: 
  rmdformats::readthedown:
    code_folding: show
    highlight: tango
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# KJ 6.3

A chemical manufacturing process for a pharmaceutical product was discussed in Sect. 1.4. In this problem, the objective is to understand the relationship between biological measurements of the raw materials (predictors), measurmenets of the manufacturing process (predictors), and the response of the product yield. Biological predictors cannot be changed but can be used to assess the quality of the raw material before processing. On the other hand, manufacturing process predictors can be changed in the manufacturing process. Improving product yield by 1% will boost revenue by approximately one hundred thousand dollars per batch:

## (a) 

Start R and use these commands to load the data:

```{r, message=FALSE, warning=FALSE}
library(AppliedPredictiveModeling)
library(tidyverse)
library(kableExtra)
library(e1071)
library(mice)
library(corrplot)
library(caret)
library(pls)

data(ChemicalManufacturingProcess)
```

The matrix `processPredictors` contains the 57 predictors (12 describing the input biological material and 45 describing the process predictors) for the 176 manufacturing runs. `yield` contains the percent yield for each run.

```{r}
cmp <- ChemicalManufacturingProcess
```

## (b)  

A small percentage of cells in the predictor set contain missing values. Use an imputation function to fill in these missing values. 

```{r}
cat(sprintf("Total NAs\n%d",sum(is.na(cmp))))
```

Columns with NAs, 

```{r}
colSums(is.na(cmp)) |> 
  data.frame() |>
  rename("NAs" = 1) |>
  filter(NAs > 0) |>
  arrange(desc(NAs)) |>
  kable()
```

Imputing values using the `mice` packages predictive mean matching -- the function provided in the book (`impute.knn` function in the `impute` package) -- generated a series of errors I was unable to resolve.  

```{r, warning=FALSE, cache=TRUE}
set.seed(1)

# store mids object (multiple imputations by chained equations)
imputed_vals <- mice(cmp, method = "pmm", m = 5, printFlag = F)
# return complete dataset
trans <- complete(imputed_vals)

# check NAs
sprintf("There are %d NA's",sum(is.na(trans)))
```

## (c)  

Split the data into a training and a test set, pre-process the data, and tune a model of your choice from this chapter. What is the optimal value of the performance metric?

Before splitting the data, we want to a look at the distribution and correlation amongst the predictors. To do this, we'll calculate the skewness statistic and examine a correlation plot of the variables, 

```{r}
skew.vals <- apply(trans, MARGIN = 2, skewness) |>
                  data.frame() |>
                  rename("skew" = 1) |>
                  filter(skew > 2 | skew < -2) |>
                  arrange(desc(skew))

sprintf("Number of skewed variables: %d",nrow(skew.vals))
kable(skew.vals)
```


Generally, a skewness between -0.5 and 0.5 indicates a relatively small amount of skewness (0 = perfect symmetry). Any values lower than -2 or greater than 2 are considered skewed. A positive skew indicates a right-skew, while a negative skew indicates a left-skew. 


In order to reduce skewness we can use a Box-Cox transformation depending on the approach we take. Now we take a look at a correlation matrix, 

```{r, warning=FALSE}
trans |>
  select(-Yield) |>
  cor() |>
  round(2) |>
  corrplot::corrplot(method = "square",
            order = "alphabet",
            tl.cex = 0.3,
            type = "lower")
```

We can see there's quite a bit of correlation amongst the predictors. A Partial Least Squares model may be in order. 

```{r}
set.seed(1)

# takes a vector of data as the first argument to return a list of indices
training_rows <- createDataPartition(trans$Yield,
                                     p = .80,
                                     list = FALSE)

# isolate the skewed variables to transform
skew_vals <- trans |>
  dplyr::select(row.names(skew.vals))

# apply Box-Cox transformation to skewed variables
preP <- preProcess(skew_vals, method = c("BoxCox"))

# create new dataframe with transformed variables, add Yield
df <- predict(preP, trans)
df$Yield <- cmp$Yield

# subset the data into objects for training using integer subsetting
train.set <- df[training_rows,]
test.set <- df[-training_rows,]
```

First we create the training and testing sets, then we pre-process the data. I am going to try the Partial Least Squares model, which we can preprocess as we fit the model. The optimal value for this is the number of components which minimizes the RMSE.

```{r, warning=FALSE}
set.seed(1)
# specify type of resampling 
ctrl <- trainControl(method = "cv", number = 10)

# set aside predictors and response 
train.x <- train.set |> dplyr::select(-Yield)
train.y <- train.set$Yield

test.x <- test.set |> dplyr::select(-Yield)
test.y<- test.set$Yield

# tune and fit the model with pre processing 
plsTune <- train(train.x, train.y,
                 method = "pls",
                 tuneLength = 20,
                 trControl = ctrl,
                 preProc = c("center","scale"))

plsTune
```

```{r}
optimal.model <- plsTune$bestTune[['ncomp']]
optimal.rmse <- plsTune$results[['RMSE']][[4]]

cat(sprintf("The optimal number of components to minimize the RMSE is %i\nThe RMSE using %i components is %f.",
            optimal.model,
            optimal.model,
            optimal.rmse))
```

```{r}
plot(plsTune)
```


## (d)   

Predict the response for the test set. What is the value of the performance metric and how does this compare with the resampled performance metric on the training set?  

```{r}
set.seed(1)
preds <- predict(plsTune, test.x)
postResample(preds, test.y)
```

The RMSE value is actually lower on the test set than it was on the training set. 

```{r}
plot(residuals(plsTune), ylab='residuals')
abline(h = 0, col = 'red')
```

The residuals appear to be random and centered around mean zero. The model appears to fit well.  


## (e)   

Which predictors are most important in the model you have trained? Do either the biological or process predictors dominate the list?  

The `varImp` function in the `caret` package calculates variable importance for objects produced by the `train` function. 

```{r}
# variable importance
var.imp <- varImp(plsTune)
plot(var.imp, top = 10)
```

The top 8 most important variables are all process predictors. 

## (f)   

Explore the relationships between each of the top predictors and the response. How could this information be helpful in improving yield in future runs of the manufacturing process? 

Let's use the ten most important variables for this exercise, 

```{r}
# store 10 most important variable names
m.imp <- var.imp$importance |>
  data.frame() |>
  arrange(desc(Overall)) |>
  head(10) |>
  row.names()

# correlation matrix of 10 most important variables and response variable
df |>
  dplyr::select(all_of(c("Yield",m.imp))) |>
  cor() |>
  round(2) |>
  corrplot::corrplot(method = "square",
            order = "alphabet",
            tl.cex = 0.6,
            type = "lower")
```

We can observe that we might reduce the processes with strong negative correlation with the yield - Manufacturing Processes 13, 17, and 36 - as reducing those processes will increase the yield. 
