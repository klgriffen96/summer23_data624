---
title: "HW"
author: "Alice Friedman"
date: "2023-06-07"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(fpp2)
```
## Week 1
### 2.1


```{r 2.1,eval=FALSE}
help(gold)
help("woolyrnq")
help("gas")
```

a. Autoplot 'gold' 'woolrnq' and 'gas'
```{r 2.1a}
autoplot(gold)
autoplot(woolyrnq)
autoplot(gas)
```

b.What is the frequency of each series?
```{r 2.1b}
frequency(gold)
frequency(woolyrnq)
frequency(gas)
```

c. Use which.max() to spot the outlier in the gold series?
```{r 2.1c}
which.max(gold)
```

### 2.3

a. Download the file tutel.csv from the book website (read in from GitHub). 

```{r 2.3a}
library(readxl)
tutel <- readxl::read_excel("retail.xlsx", skip=1)
```


b. Convert to timeseries -- drop the first column which contains dates.

```{r 2.3b}
tutelts <- ts(tutel[,-1], start=1981, frequency=4)
```

c. Construct timeseries plots of each of the three series
```{r 2.3c}
autoplot(tutelts, facets = TRUE)
```

### 6.2

a. Plot the time series of sales of Product A
```{r 6.2a}
#autoplot(plastics)
plastics %>% decompose(type="multiplicative") %>% autoplot()

```

b Calculate the trend-cycle and the seasonality using multiplicative decomp
```{r 6.2b}
#does calculate indicate something other than plot?
plastics %>% stl(t.window=4, s.window="periodic", robust = TRUE) %>% autoplot()
```
c. Do results support the graphical interpretaion?
```{r 6.2c}
??
```
d. Plot seasonally adjusted data
```{r 6.2d}
fit <- mstl(plastics)
fit %>% seasadj()%>%autoplot()
```

e + f. Add an outlier -- recompute. What is the effect?

The location of the outlier affects the location of the anamoly

```{r 6.2ef}

outlier <- function(n, v){
  myplastics <- plastics
  myplastics[[n]] <- v
  myfit <- mstl(myplastics)
  myfit %>% seasadj()%>%autoplot()
}

outlier(1, 500)

outlier(30, 500)

outlier(60, 50)
```
## Week 2


### HA 7.1

Consider the `pigs` series -- the number of pigs slaughtered in Victoria each month.

### 7.1 a 

Use the `ses` function to find the optimal levels of alpha and l-zero, and generate forecasts for the nexy 4 months.

```{r 7.1a}
fit_pigs <- pigs %>% ses(h=4)

fit_pigs %>% autoplot()

summary(fit_pigs)
```
### 7.1 b
 Compute a 95% prediction interval for the first forecast using y-hat is plus or minus 1.96s where `s` is the standard deviation of the residuals. Compare your interval with the interval produced by R.
```{r 7.1b}
me <- 1.96*sd(fit_pigs$residuals)

f1 <- fit_pigs$mean[1]


```

The first forecast is `r f1` with a 95% margin of error plus or minus `r me`. 

```{r 7.1b_table}
library(dplyr)
labels_7.1b <- c("My Results", "R")
my_pigs <- c(f1-me, f1+me)
R_pigs <- c(fit_pigs$lower[5], fit_pigs$upper[5])
df_pigs <- data.frame(my_pigs, R_pigs)
colnames(df_pigs) <- labels_7.1b
row.names(df_pigs) <- c("Lower 95% Bound", "Upper 95% Bound")
df_pigs <- df_pigs %>% mutate(
  `% Diff` = (`My Results` - R)/R*100
)

df_pigs %>% t()
```

### HA 7.3
(In the previous exercise, we we asked to write our own function to implement simple exponential smoothing.)

Modify your function from the previous exercise to return the sum of the squared errors rather than the forecast of the next observations. The use the `optim` function to find the optimal methods of alpha and l-0. Do you get the same values as the `ses()` function?

There is not difference depending on the level!

```{r 7.2}
T <- length(pigs) %>% as.numeric()
my_smooth <- function(y, alpha, level) {
  
  T <- length(y) %>% as.numeric()
  
  my_fitted <- c()
  
  #set first value 
  my_fitted[1] <- level

    # I really wish we could do this wihout a for loop!!!
    # Also I don't understand why this doesn't have the exponential or summation??
    for (i in 2:(T+1)) {
    my_fitted[i] <- alpha * y[i-1] + (1 - alpha) * my_fitted[i-1]
  }
  
  #return last value
  return(my_fitted)
  }

alpha <- 0.291
level <- pigs[1]

my_smooth(pigs, alpha=alpha, level=level)[T+1]
my_smooth(pigs, alpha=alpha, level=level/2)[T+1]

ses(pigs, alpha=alpha)$mean[1]

```


```{r 7.3}
my_sse <- function(y, alpha, level) {
  
  y_hats <- my_smooth(y, alpha, level)
  resid <- y_hats - y
  sse <- mapply(function(x) x^2, resid) %>% sum()

  return(sse)

}

my_sse(pigs, alpha, level)

my_sse2 <- function(y, alpha, level) {
  
  y_hats <- ses(y, alpha=alpha)$mean[1:T]
  resid <- y_hats - y
  sse <- mapply(function(x) x^2, resid) %>% sum()

  return(sse)

}

my_sse2(pigs, alpha, level)

```

## KJ 3.1
```{r 3.1}
library(mlbench)
data(Glass)
str(Glass)
```
### 3.1a
Using visualizations, explore the predictor variables to understand their distributions as well as the relationships between predictors.

```{r}
boxf <- function(key){
    p <- ggplot(Glass, 
                aes(factor(Type), {{key}})) + 
                           geom_boxplot() + 
                           xlab("Type")

  print(p)
}

cols <- names(Glass) %>% head(-1)


  boxf(col[1])

```


### 3.1b
Do their appear to be any outliers? Are any predictors skewed?

```{r 3.1b}
library(psych)
describe(Glass)

```

## KJ 3.2



```{r ref.label=knitr::all_labels(), echo=TRUE, eval=FALSE}
```
