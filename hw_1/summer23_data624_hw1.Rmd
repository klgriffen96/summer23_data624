---
title: 'Data 624: Predictive Analytics HW 1'
author: "Group 2: Alice Friedman, Kayleah Griffen, Josh Iden, Michael Ippolito"
date: "6/18/2023"
output:
  html_document:
    df_print: paged
  word_document: default
always_allow_html: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
```
## Introduction

This homework assignment includes problems from:

(1) Hyndman & Athanasopoulos. "Forecasting: Principles and Practice"
(2) Kuhn & Johnson. "Applied Predictive Modeling"

This accompanies readings from KJ 1,2 and 3 and HA 1,2,6,7 and 8.

## Week 1 Homework Solutions

### HA 2.1

Using the help function (`?`) to explore what the series `gold`, `woolyrnq` and `gas` represent, we get the following results in the description:

?gold
 "Daily morning gold prices in US dollars. 1 January 1985 – 31 March 1989"
 
?woolyrnq
"Quarterly production of woollen yarn in Australia: tonnes. Mar 1965 – Sep 1994"
 
?gas
Australian monthly gas production: 1956–1995.


#### a. Use autoplot() to plot each of these in separate plots.

```{r 2.1a, warning = FALSE, message=FALSE}
library(fpp2)
library(gridExtra)
library(tidyverse)



p1 = autoplot(gold) + ggtitle('Price of Gold: 1985-1989') + ylab('price') + xlab('')
p2 = autoplot(woolyrnq) + ggtitle('Quarterly Production of Woollen Yarn in Australia: 1965 - 1994') + ylab('tonnes') + xlab('')
p3 = autoplot(gas) + ggtitle('Australian monthly gas production: 1956-1995') + ylab('gas') + xlab('')

grid.arrange(p1, p2, p3, nrow=3)
```

#### b. What is the frequency of each series? 

```{r 2.1_frequency}

Gold <- frequency(gold)

Wool <- frequency(woolyrnq)

Gas <- frequency(gas)

Freqs <- data.frame(
  Series = c("Gold", "Wool", "Gas"),
  Frequency = c(Gold, Wool, Gas),
  Seasonality = c("Daily", "Quaterly", "Monthly")
  ) 
Freqs %>% print() 
```

Based on `frequency` and the help function you can tell the `gold` observations are taken daily, the `woolyrnq` is quarterly, and the `gas` is monthly.


#### c. Use which.max() to spot the outlier in the gold series. Which observation was it?

`which.max` determines the index of the maximum of the numeric vector. This occurs at day 770 in the gold series. We can check what date this is using `lubridate` package.

```{r, warning=FALSE, message = FALSE}
library(lubridate)

d <- ymd("1985/01/01") + which.max(gold)

paste0('day:', which.max(gold) ,  '  date:', d, '  price: $', gold[which.max(gold)])

```
The max price of gold occurred on February 10, 1987.

### HA 2.3

#### a. Download some monthly Australian retail data from the book website (https://otexts.com/fpp2/extrafiles/retail.xlsx). These represent retail sales in various categories for different Australian states, and are stored in a MS-Excel file.

Head of the downdloaded dataset is below.

```{r 2.3setup}
library(readxl)
retaildata <- readxl::read_excel("retail.xlsx", skip=1)
head(retaildata)
```

#### b. Select one of the time series as follows (but replace the column name with your own chosen column):

```{r 2.3b}
myts <- ts(retaildata[,"A3349335T"],
  frequency=12, start=c(1982,4))
```

#### c. Explore your chosen retail time series using the following functions: `autoplot()`, `ggseasonplot()`, `ggsubseriesplot()`, `gglagplot()`, `ggAcf()`

Below we plotted `autoplot()`, `ggseasonplot()`, `ggsubseriesplot()`, & ` ggAcf()` in a grid. The lag plot requires a lot of space so it is plotted seperately!

```{r 2.3}
ap <- autoplot(myts) +
  ggtitle("Autoplot: Monthly retail sales in various categories for different Australian states") +
  xlab("Year") +
  ylab("Retail Sales")

sp <- ggseasonplot(myts, year.labels=TRUE, year.labels.left=TRUE) +
  ylab("Retail Sales") +
  ggtitle("Seasonal plot: Monthly retail sales")

gsp <- ggsubseriesplot(myts) +
  ylab("Retail Sales") +
  ggtitle("Subseries Plot: Monthly retail sales")

myts_window <- window(myts, start = c(1982, 4))
win <- ggAcf(myts_window)


grid.arrange(ap, sp, gsp, win, ncol=2)
```

```{r}
lagged <- gglagplot(myts_window) + 
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1)) + 
  ggtitle("Lag Plot")
print(lagged)
```

#### Can you spot any seasonality, cyclicity and trend? What do you learn about the series?

* From the autoplot, we can see that there is a clear, increasing trend, as well as seasonality. 

* From the seasonal trend, it is easier to see that the seasonality has an annual frequency which is additionally following a steady upward trend over time.

* The subseries plot again shows the trend clearly -- within each month there is a steady upward trend that appears to have roughly the same slope every month indicating that the trend is consistent across years.

* The window plot shows a clear trend (rather than white noise) which indicates that there is strong autocorrelation in the series.

* Finally, the lagged scatterplots show that the relationships are strongly positive across all lags, but dramatically positive at lag 12. This indicates a high annual correlation. We can see this in the ACF plot as well. We also notice that the correlation, while remaining positive, becomes less strongly positive as time goes by, and then peaks again every 12 months, again pointing to the 12 monthly frequency.


### HA 6.2

The `plastics` data set consists of the monthly sales (in thousands) of product A for a plastics manufacturer for five years.

#### a. Plot the time series of sales of product A. Can you identify seasonal fluctuations and/or a trend-cycle?

```{r 6.2a}
# ?plastics
# "Monthly sales of product A for a plastics manufacturer."

autoplot(plastics) +
  ggtitle("Monthly sales of product A for a plastics manufacturer") +
  xlab("Year") +
  ylab("Sales")
```

Based on the autoplot there does appear to be seasonality to the data, the plastic sales are highest May - October (peaking in August usually) and lower November - April (lowest in February). There is a consistent upward trend-cycle as well. 

#### b. Use a classical multiplicative decomposition to calculate the trend-cycle and seasonal indices.

We use the `decompose` function with `type="mutliplicative"` and then `autoplot` to decompose the series and then evalaute the results.

```{r 6.2b}
plastics %>% decompose(type="multiplicative") %>%
  autoplot() + xlab("Year") +
  ggtitle("Classical multiplicative decomposition
    of plasatic sales")
```

The trend cycle shows a strong seasonal component, with a yearly frequency, and has an increasing trend up until just past year 5 when it begins decreasing. There is some remainder as well. The remainder values below 1 indicate that there is some “leakage” of the trend-cycle component into the remainder component - the trend-cycle estimate has over-smoothed the drop in data.

#### c. Do the results support the graphical interpretation from part a?

Yes, the graphical interpretation from part A aligns with the multiplicative decomposition. The yearly seasonal pattern was noted in both a and b. The only part that was not captured well is the drop off in the trend after year 5. 

#### d. Compute and plot the seasonally adjusted data.

We can compute the seasonally adjusted fit by applying the `seasadj` function to the decomposed time series.

```{r 6.2d, warning=FALSE}
fit <- plastics %>%
  decompose(type="multiplicative") 

autoplot(plastics, series="Data") +
  autolayer(trendcycle(fit), series="Trend") +
  autolayer(seasadj(fit), series="Seasonally Adjusted") +
  xlab("Year") + ylab("Sales") +
  ggtitle("Monthly sales of product A for a plastics manufacturer") +
  scale_colour_manual(values=c("gray","blue","red"),
             breaks=c("Data","Seasonally Adjusted","Trend"))

```

#### e. Change one observation to be an outlier (e.g., add 500 to one observation), and recompute the seasonally adjusted data. What is the effect of the outlier?

Our approach is to create a funciton that allows us to quickly check different outliers and positions, then compare several outputs to the original seasonally adjusted plot.

```{r 6.2e, warning=FALSE}

#Function that creates plots based on the position (n) value (v) and whether it's an outlier or not (outlier)
outlier <- function(n, v, outlier){
  
  # if it's an ourlier, add the outlier and change the plot subtitle
  if(outlier==TRUE){
    plastics[n] <- v
    subtitle <- paste("Outlier is", v, "at month", n)
  } else {
    subtitle <- "Original data"
  }
  
  #fit to the decomp
  fit <- plastics %>% decompose(type="multiplicative") 
  
  #plot
  autoplot(plastics, series="Data") +
    autolayer(seasadj(fit), series="Seasonally Adjusted") +
    xlab("Year") + ylab("Sales") +
    labs(
      title = "Monthly sales of product A for a plastics manufacturer",
      subtitle = subtitle
      ) +
    scale_colour_manual(values=c("gray","blue"),
               breaks=c("Data","Seasonally Adjusted","Trend")) + 
    scale_y_continuous(expand = c(0, 0), limits = c(0, NA))

  
}

# plot the original and two variations in a grid
grid.arrange(
  outlier(1, 1, FALSE),
  outlier(59, 50, TRUE),
  outlier(30, 50, TRUE), 
  ncol=1)

```

The outlier in the middle causes the seasonally adjusted fit to hallucinate seasons that don't exist! At or near the end, there is no discernible effect on the overall seasonally adjusted fit.

#### f. Does it make any difference if the outlier is near the end rather than in the middle of the time series?

If the outlier is at the end vs in the middle, there is more of an effect on the seasonally adjusted data. The estimate of the trend cycle is unavailable for the first and last few observations - without this there is also no estimate of the remainder component. Due to this, all of the outlier is passed on to the seasonally adjusted rather than having some of it put in the remainder or in the trend.  If the outlier is in the middle some of the variance is put into the cycle component and causes some "hallucinated" peaks

## Week 2 Homework Solutions

### HA 7.1 Consider the `pigs` series -- the number of pigs slaughtered in Victoria each month.

### 7.1 a Use the `ses` function to find the optimal levels of alpha and l-zero, and generate forecasts for the next 4 months.

```{r 7.1a}
fit_pigs <- pigs %>% ses(h=4)

fit_pigs %>% autoplot()

#summary(fit_pigs)
```
By callng `summary` on the results of the `ses` object, we can see that the optimal values of alpha and $ℓ_0$ are 0.2971 and 77260.0561, respectively.

The first four predicted values are given by calling the `mean` operator on the `summary` object, as shown below. 

```{r 7.1aforecasts}
summary(fit_pigs)$mean
```

### 7.1 b Compute a 95% prediction interval for the first forecast using y-hat is plus or minus 1.96s where `s` is the standard deviation of the residuals. Compare your interval with the interval produced by R.

```{r 7.1b}
# margin of error
me <- 1.96*sd(fit_pigs$residuals)

# first forecast
f1 <- fit_pigs$mean[1]
```

The first forecast from the `ses` prediction on pigs is `r round(f1, 2)` with a 95% margin of error plus or minus `r round(me, 2)`. A comparison to results from R is shown below. The results are within 1/10 of 1% from the manually calculated values.

```{r 7.1b_table}
library(dplyr)

#Make a table to review results
labels_7.1b <- c("My Results", "R")

df_pigs <- data.frame(
  c(f1-me, f1+me), 
  c(fit_pigs$lower[5], fit_pigs$upper[5]))

colnames(df_pigs) <- labels_7.1b
row.names(df_pigs) <- c("Lower 95% Bound", "Upper 95% Bound")
df_pigs <- df_pigs %>% mutate(
  `% Diff` = round((`My Results` - R)/R*100,2)
)

df_pigs %>% t()
```


### HA 7.3

#### Modify your function from the previous exercise to return the sum of the squared errors rather than the forecast of the next observations. The use the `optim` function to find the optimal methods of alpha and $ℓ_0$. Do you get the same values as the `ses()` function?

Note: In the previous exercise, we were asked to write our own function to implement simple exponential smoothing. This is shown below.

```{r 7.2}
T <- length(pigs) %>% as.numeric()

my_smooth <- function(y, alpha, level) {
  
  T <- length(y) %>% as.numeric()
  
  my_fitted <- c()
  
  #set first value 
  my_fitted[1] <- level

    for (i in 2:(T+1)) {
    my_fitted[i] <- alpha * y[i-1] + (1 - alpha) * my_fitted[i-1]
  }
  
  #return last value
  return(my_fitted)
  }

alpha <- 0.291
level <- pigs[1]

my_smooth(pigs, alpha=alpha, level=level)[T+1]
my_smooth(pigs, alpha=alpha, level=level/2)[T+1]
my_smooth(pigs, alpha=1/T, level=level)[T+1]
my_smooth(pigs, alpha=1/T, level=level/2)[T+1]

ses(pigs, alpha=alpha)$mean[1]

```

The optim() function using the default method (Nelder and Mead) produced different values for α and $ℓ_0$ (0.5179980 and 0.1307012, respectively). Changing the method to BFGS (Broyden, Fletcher, Goldfarb and Shanno) made the results very close to the one produced by the ses() function (0.297142 and 77273.160331, respectively).

```{r}
# Function to implement SES, modified to return the sum of squared errors instead of the forecast of the next observation
my_ses_mod <- function(y, params) {

    # Split out parameters for optim() function    
    alpha <- params[1]
    level <- params[2]
    
    # Initialize vector to store forecast values
    fitted <- c()
    fitted[1] <- level
    
    # Set T to be length of time series
    T <- length(y)
    
    # Iterate over terms
    for (j in seq(2, T)) {
        fitted[j] <- alpha * (y[j - 1]) + (1 - alpha) * fitted[j - 1]
    }
    
    # Convert fitted values to time series
    fitted <- ts(fitted, frequency=12, start=c(1980, 1))
    
    # Calculate residuals
    residuals <- y - fitted
    
    # Calculate sum of squared errors
    sse=sum(residuals ** 2)

    # Return fitted values, residuals, and sum of squared errors
    # Return sse
    return(sse)

}

# Calculate SSE using modified custom function
fit2 <- my_ses_mod(pigs, c(0.2971, 77260.0561))

# Compare this SSE to SSE from original fit using sse()

ans7.1b <- data_frame(
  MyFit = c(fit2),
  sesFit = c(sum(fit_pigs$residuals ** 2))
)

ans7.1b <- ans7.1b %>% 
  mutate(
  `% Diff` = round((MyFit - sesFit)/sesFit*100 , 2)
)

print(ans7.1b)

# Optimize α and ℓ_0 using optim() function
fit3 <- optim(par=c(0, 0), fn=my_ses_mod, y=pigs)
fit3$par  # produces alpha=0.5179980, l_0=0.1307012

# Try optimize again using method='BFGS'
fit4 <- optim(par=c(0, 0), fn=my_ses_mod, y=pigs, method='BFGS')
fit4$par


```


## KJ 3.1

The UC Irvine Machine Learning Repository6 contains a data set related
to glass identification. The data consist of 214 glass samples labeled as one
of seven class categories. There are nine predictors, including the refractive
index and percentages of eight elements: Na, Mg, Al, Si, K, Ca, Ba, and Fe.
The data can be accessed via:
> library(mlbench)
> data(Glass)
> str(Glass)

```{r 3.1}
library(mlbench)
data(Glass)
```

### 3.1a
Using visualizations, explore the predictor variables to understand their distributions as well as the relationships between predictors.

```{r 3.1a_pLots, message=FALSE, warning=FALSE}

# Gather
Glass_long <- Glass %>% select(-Type) %>% gather() 

# Hist plots
Glass_long %>% ggplot(aes(value)) + geom_histogram() + facet_wrap(~key, scales = "free")

# Box plots
Glass_long %>% ggplot(aes(y=value))+geom_boxplot()+facet_wrap(~key, scales = "free")
  
```

A colorized correlation plot is used to understand relationships between predictors.

```{r 3.1a_corrPLots}
library(corrplot)
library(caret)
# Filter for just quantitative vars
glassQuant <- Glass %>% select (-Type)

glasscorr <- cor(glassQuant)
# Corr chart
corrplot(cor(glasscorr), method="number", order = "AOE", type = "lower", diag = FALSE)

# Find correlations past cutoff
print(paste0("Candidate for removal due to high correlation: ", findCorrelation(glasscorr, cutoff=0.8, exact=T, verbose=T, names=T)))
```

There is some collinearity between `RI` and `Ca`. There is some correlation between a few of the other variables: `RI` and `Si`, `Mg` and `AI`, and `Ba` and `AI`.

The RI-Ca correlation is the only one to take particular note of, as it could impact the types of modeling sensitive to those effect (e.g. linear models). Using the heuristic algorithm proposed by K&J (and using the `findCorrelation()` function), calcium is a candidate for removal due to high correlation. 

All variables except Type are quantitative; Type is a categorical variable with 6 levels (1, 2, 3, 5, 6, and 7).

### 3.1b Do there appear to be any outliers? Are any predictors skewed?

As we observed from the histograms, all of the predictors are skewed to some degree, but the worst offenders are `Mg`, which appears to be bimodal, and `K`, `Ba`, and `Fe`, which appear to be right-skewed. We can confirm this by using the `skewness` function:

```{r 3.1b}
library(psych)
describe(Glass) %>% select(skew)
```

As far as outliers, let's take a look the boxplots above. There are a lot of outliers present (greater than 1.5X outside the interquartile range) in each predictor with the exception of `Mg`.

## KJ 3.2

### KJ 3.2 (a)

The soybean data can also be found at the UC Irvine Machine Learning Repository. Data were collected to predict disease in 683 soybeans. The 35 predictors are mostly categorical and include information on the environmental conditions (e.g., temperature, precipitation) and plant conditions (e.g., left spots, mold growth). The outcome labels consist of 19 distinct classes.

**Investigate the frequency distributions for the categorical predictors. Are any of the distributions degenerate in the ways discussed earlier in this chapter?**

Using the nearZeroVar() function, these variables appear to be degenerate as described in K&J as having near-zero variance:

* leaf.mild
* mycelium
* sclerotia

As shown in the summary output, each of these columns has a dominant class that far exceeds its next most populous class.

```{r}

# Load data
data(Soybean)
#str(Soybean)

# Generate frequency tables to look for degenerate distributions
multifunc2 <- function(x) {
    return(prop.table(table(x)))
}

# Degenerate vars: if both of these are true:
# 1. fraction of unique vals over the sample size < 10%
# 2. ratio of freq of the most prevalent to the freq of the second most prevalent is > 20
nzv <- nearZeroVar(Soybean)
#print(colnames(Soybean[,nzv]))

# Examine NZV columns
summary(Soybean[,nzv])

```

### KJ 3.2 (b)

**Roughly 18% of the data are missing. Are there particular predictors that are more likely to be missing? Is the pattern of missing data related to the classes?**

As shown in the md.pattern plot below, there are 562 (of 683) complete cases, or roughly 82% of the data. Of the 18% of the data that have over 15% missing values, most of the variables are related to plant pathology:

* hail
* sever
* seed.tmt
* germ
* leaf.mild
* lodging
* fruiting.bodies
* fruit.spots
* seed.discolor

We verified that the missing values weren't due to the absence of the specific pathology, as negative cases were also reported. So the values do appear to be missing as opposed to being intentionally or structurally absent.

```{r fig.width=6, fig.height=6}
col_sums = colSums(is.na(Soybean))

data.frame(variable = names(col_sums), missing = col_sums, row.names = NULL) |> 
  ggplot(aes(y=reorder(variable,missing), x=missing)) +
  geom_col() +
  ggtitle('Missing Values') + xlab('') + ylab('')

```

We see a bunch of predictors with more than 80 NAs, related to seed (`seed.tmt`,`seed.size`,`seed.discolor`,`seed`), leaf information (`leaf.size`,`leaf.shread`,`leaf.mild`,`leaf.marg`,`leaf.malf`,`leaf.halo`), fruit (`fruiting.bodies`,`fruit.spots`,`fruit.pods`), and a few others that are not of a clear category (`shriveling`,`sever`,`mold.growth`,`lodging`,`hail`,`germ`).

Checking to see how these NAs relate by class:

```{r}
Soybean |>
  group_by(class = Class) |>
  summarize(total_na = sum(is.na(across(-Class))),
            pct_na=sum(is.na(across(-Class)))
            / n()) |>
  arrange(desc(pct_na))
```

We see that five of the 19 classes are responsible for all of the NAs. This indicates to me a likelihood that certain measurements are either applicable or not available to these classes. This should be taken into consideration when considering how to deal with these missing values.

### KJ 3.2 (c)

**Develop a strategy for handling missing data, either by eliminating predictors or imputation.**

The degenerate values should be removed as they offer no predictive value.

Because NA values only appear in 5 of the 14 classes, missingness is potentially informative in predicting a class. Two ways to deal with this while retaining the predictive value of the missingness is to **1)** create an **indicator variable** that represents the presence or absence of missing values in a specific variable, or **2)** create a **missingness pattern variable** that indicates whether a specific combination of variables is missing or not.

It's also possible to apply pattern-based imputation using the `mice` or `amelia` packages in R.

```{r}
# Create copy of the original data set
sb2 <- Soybean

# Remove the observations in which 19 or more variables have missing values
most_complete <- rowSums(is.na(sb2)) < 19
sb2 <- sb2[most_complete,]

# Recalculate number of rows contain missing values
some_missing <- rowSums(is.na(sb2)) > 0
print(paste0("Proportion of observations with missing values after removing those with high proportion of missing variables: ", 
    round(nrow(sb2[some_missing,]) / nrow(sb2), 3)))
print(paste0("Proportion of observations retained from original data set: ",
    round(nrow(sb2) / nrow(Soybean), 3)))

```

An alternative approach would be to remove the three variables with degenerate classes:

* leaf.mild
* mycelium
* sclerotia

and to additionally remove those with a high proportion of data:

* hail
* sever
* seed.tmt
* germ
* leaf.mild (also degenerate)
* lodging
* fruiting.bodies
* fruit.spots
* seed.discolor

However, that still leaves 17.7% of observations with at least one missing value. So a combination of removing some observations along with the above variables would be more effective. The disadvantage of this is that 11 variables would be removed--a full 30% of all variables.

```{r}
# Create copy of original data set
sb3 <- Soybean

# Remove the observations in which 24 or more variables have missing values
most_complete <- rowSums(is.na(sb3)) < 24
sb3 <- sb3[most_complete,]

# Remove variables with degenerate classes or having high proportion of missing values (11 all together)
sb3 <- sb3 %>%
    select(-leaf.mild, -mycelium, -sclerotia, -hail, -sever, -seed.tmt, -germ, -lodging, -fruiting.bodies, -fruit.spots, -seed.discolor)

# Recalculate number of rows contain missing values
#some_missing <- rowSums(is.na(sb3)) > 0
#print(paste0("Proportion of observations with missing values after removing degenerate variables: ", 
#    round(nrow(sb3[some_missing,]) / nrow(sb3), 3)))
#print(paste0("Proportion of observations retained from original data set: ",
#    round(nrow(sb3) / nrow(Soybean), 3)))

```


Another approach would be to impute the missing data. The MICE package (multivariate imputation by chained equations) is an appropriate way to do this. Even using MICE, the observations with over 24 variables missing would seem to remain problematic, so we'll remove those first before imputing. As shown below, we now have a dataset with no missing cases, and over 95% of the original observations were retained.


```{r warning=FALSE, eval=FALSE}

# Create copy of original data set
sb4 <- Soybean

# Remove the observations in which 24 or more variables have missing values
most_complete <- rowSums(is.na(sb4)) < 24
sb4 <- sb4[most_complete,]

# Recalculate number of rows contain missing values
some_missing <- rowSums(is.na(sb4)) > 0
print(paste0("Proportion of observations with missing values after removing degenerate variables: ", 
    round(nrow(sb4[some_missing,]) / nrow(sb4), 3)))
print(paste0("Proportion of observations retained from original data set: ",
    round(nrow(sb4) / nrow(Soybean), 3)))

# Impute missing data
imp <- mice(sb4, maxit=5, m=5, seed=777)

# Complete the data set using imputed values
sb4 <- complete(imp)

# Recalculate number of rows contain missing values
some_missing <- rowSums(is.na(sb4)) > 0
print(paste0("Proportion of observations with missing values after removing degenerate variables: ", 
    round(nrow(sb4[some_missing,]) / nrow(sb4), 3)))
print(paste0("Proportion of observations retained from original data set: ",
    round(nrow(sb4) / nrow(Soybean), 3)))

```


## Week 3 Homeowork Solutions


### HA 8.1

#### HA 8.1 (a)

Figure 8.31 shows the ACFs for 36 random numbers, 360 random numbers and 1,000 random numbers.

**Explain the differences among these figures. Do they all indicate that the data are white noise?**

*Figure 8.31: Left: ACF for a white noise series of 36 numbers. Middle: ACF for a white noise series of 360 numbers. Right: ACF for a white noise series of 1,000 numbers.*

According to the book, “autocorrelation measures the linear relationship between lagged values of a time series,” and the autocorrelation coefficients are plotted to show the autocorrelation function (ACF) in a plot called a correlogram. If there is no autocorrelation, this is called white noise and the autocorrelation should be close to 0. To qualify as white noise, 95% of the spikes in the ACF need to be within $\pm\ \frac{1.96}{\sqrt{T}}$ where T is the length of the time series. The bounds are plotted as the blue dashed lines on the correlogram.

The difference among the figures is the length of the time series - that is why the blue dashed lines are at different values. For the longer time series, the bounds are the smallest, and for the shortest time series the bounds are the largest. Because more than 95% of the spikes lie within the bounds for each of the figures, they all indicate white noise. Series X2 does appear to have potentially two spikes just outside of the bounds, but we still qualified it as white noise because some of the other spikes are almost too small to see, and therefore we estimated that 95% of the spikes still lie within the bounds.

#### HA 8.1 (b)

**Why are the critical values at different distances from the mean of zero? Why are the autocorrelations different in each figure when they each refer to white noise?**

The critical values are those that occur above or below $\pm\frac{1.96}{\sqrt{T}}$, where T is the number of samples in the series. Since the denominator (square root of sample size) increases for each series while the numerator is held constant, the critical value will grow narrower as T gets larger. Moreover, the autocorrelations are different in each figure because each figure represents random numbers, therefore the autocorrelations are different from one another.

### HA 8.2

**A classic example of a non-stationary series is the daily closing IBM stock price series (data set ibmclose). Use R to plot the daily closing prices for IBM stock and the ACF and PACF. Explain how each plot shows that the series is non-stationary and should be differenced.**

As shown below, the ACF plot exhibits a downward decay consistent with autocorrelated values for each successive lag. If the series were stationary, the ACF plot would exhibit a drop to zero very soon after lag 1. The PACF only exhibits a single spike above significance at lag 1, indicating that an ARIMA (1,1,0) model would be appropriate. The PACF would indicate an AR(1) model which will turn out to be equivalent to taking a first difference > indicating that it does need differencing. 

```{r}

# Load data
data(ibmclose)

# Plot
ggtsdisplay(ibmclose, main='Daily closing IBM stock price series')

```

### HA 8.6

#### HA 8.6 (a)

Use R to simulate and plot some data from simple ARIMA models.

**Use the following R code to generate data from an AR(1) model with $ϕ_1$ = 0.6 and $σ^2$ = 1. The process starts with $y_1$ = 0.**

y <- ts(numeric(100))
e <- rnorm(100)
for(i in 2:100)
  y[i] <- 0.6*y[i-1] + e[i]

```{r}

# Make the R code provided in the problem into a function
genTS_AR1 <- function(phi1) {

    y <- ts(numeric(100))
    e <- rnorm(100)
    for(i in 2:100)
        y[i] <- (phi1 * y[i - 1]) + e[i]
    return(y)
}

# Generate initial time series
set.seed(777)
y <- genTS_AR1(0.6)

```

An AR(p) model is an autoregressive model with an order of p; in our case the order is 1. The order of 1 indicates how many lagged values of y are included, so in this case it is 1 lagged y value. The code above successfully generated the data from an AR(1) model with the given parameters.

#### HA 8.6 (b)

**Produce a time plot for the series. How does the plot change as you change $ϕ_1$?**

More negative values of $\phi$ make each successive value in the series more likely to have the opposite sign than the previous value, resulting in a plot that appears to fluctuate rapidly, with denser peaks and valleys. As $\phi$ approaches zero, any deviation between successive values is entirely due to random error, and therefore appears as white noise. More positive values of $\phi$ make each successive value more likely to have the same sign as the previous value, resulting in a series that appears to have wider-spaced peaks and valleys. According to the book, when you have an AR(1) and ϕ1 = 0, yt is equivalent to white noise. When ϕ1 is 1 and c = 0 you have random walk. When ϕ1 < 0, yt oscillates near the mean. These properties are observed in the plots above.

```{r}

# Generate initial time plot for series y
y %>%
    autoplot() +
    ylab('y') +
    ggtitle('AR1 time series')

```

```{r fig.width=11, fig.height=8}

# Generate additional plots, varying phi1
# (for an AR(1) model, -1 < phi < 1)
plt <- list()
phis <- seq(-0.8, 0.8, 0.2)
for (i in seq(1, length(phis))) {
    set.seed(777)
    ynew <- genTS_AR1(phis[i])
    plt[[i]] <- ynew %>%
        autoplot() +
        ylab('y') +
        ggtitle(paste0('AR1 series, phi = ', phis[i]))
}
do.call('grid.arrange', c(plt, nrow=3))

```

#### HA 8.6 (c)

**Write your own code to generate data from an MA(1) model with $θ_1$ = 0.6 and $σ^2$ = 1.**

```{r}

# Function to generate MA(1) model given theta1
genTS_MA1 <- function(theta1) {

    y <- ts(numeric(100))
    e <- rnorm(100)
    for(i in 2:100)
        y[i] <- (theta1 * e[i - 1]) + e[i]
    return(y)
}

# Generate initial time series
set.seed(777)
yma1 <- genTS_MA1(0.6)

```

#### HA 8.6 (d)

**Produce a time plot for the series. How does the plot change as you change $θ_1$?**

The peaks and valleys seem to widen as values of $\theta_1$ grow from negative to positive. This is because of the positive sign in front of the regression term $\theta_1 \epsilon_{t-1}$. Since since $\epsilon$ is a random number between -1 and 1 with a mean of 0 and standard deviation of 1, then multiplying one such number by a negative value and adding it to another random number with mean 0 and sd 1 will be more likely to result in a number with the opposite sign as the previous, resulting in the tighter peaks and valleys seen when $\theta_1$ is negative. The opposite is true when $\theta_1$ is positive: i.e., it is more likely that a number with the same sign is generated, meaning that that the current trend will continue. This results in a plot that appears to have peaks and valleys that are more spread out.

```{r}

# Generate time plot for MA1 series
yma1 %>%
    autoplot() +
    ylab('y') +
    ggtitle('MA1 time series')

```

```{r fig.width=11, fig.height=8}

# Generate additional plots, varying theta1
# (for an MA(1) model, -1 < theta < 1)
pltma1 <- list()
thetas <- seq(-0.8, 0.8, 0.2)
for (i in seq(1, length(thetas))) {
    set.seed(777)
    ynew <- genTS_MA1(thetas[i])
    pltma1[[i]] <- ynew %>%
        autoplot() +
        ylab('y') +
        ggtitle(paste0('MA1 series, theta = ', thetas[i]))
}
do.call('grid.arrange', c(pltma1, nrow=3))

```

#### HA 8.6 (e)

**Generate data from an ARMA(1,1) model with $ϕ_1$ = 0.6, $θ_1$ = 0.6 and $σ^2$ = 1.**

```{r}

# Function to generate ARMA(1,1) model given phi1 and theta1
genTS_ARMA1_1 <- function(phi1, theta1) {

    y <- ts(numeric(100))
    e <- rnorm(100)
    for(i in 2:100)
        y[i] <- (phi1 * y[i - 1]) + (theta1 * e[i - 1]) + e[i]
    return(y)
}

# Generate initial time series
set.seed(777)
yarma1_1 <- genTS_ARMA1_1(0.6, 0.6)

```

#### HA 8.6 (f)

**Generate data from an AR(2) model with $ϕ_1$ = −0.8, $ϕ_2$ = 0.3 and $σ^2$ = 1. (Note that these parameters will give a non-stationary series.)**

```{r}

# Function to generate AR(2) model given phi1 and phi2
genTS_AR2 <- function(phi1, phi2) {

    y <- ts(numeric(100))
    e <- rnorm(100)
    for(i in 3:100)
        y[i] <- (phi2 * y[i - 2]) + (phi1 * y[i - 1]) + e[i]
    return(y)
}

# Generate initial time series
set.seed(777)
yar2 <- genTS_AR2(-0.8, 0.3)

```

#### HA 8.6 (g)

**Graph the latter two series and compare them.**

The ARMA(1,1) model is stationary. Introducing an MA(1) term into an AR(1) model shouldn't (and doesn't) have any effect on producing any cyclicity, seasonality, or trend. On the other hand, the AR(2) model with the given $\phi_1$ and $\phi_2$ values does introduce stark seasonality into the plot. It is noted that when $\phi_2 - \phi_1$ > 1, similar seasonality can be produced. For example, similar plots can be produced with the following values:

| $\phi_1$ | $\phi_2$ |
|----------|----------|
| -0.9     | 0.2      |
| -0.8     | 0.3      |
| -0.6     | 0.5      |
| -0.4     | 0.7      |
| -0.2     | 0.9      |

This is because any random variation introduced by the $\epsilon$ term will be counteracted by adding the two $\phi$ terms together, which are guaranteed to have an absolute value greater than 1.

```{r fig.width=11, fig.height=6}

# Generate time plot for the ARMA(1,1)series
p1 <- yarma1_1 %>%
    autoplot() +
    ylab('y') +
    ggtitle('ARMA(1,1) time series')

# Generate time plot for the AR(2) series
p2 <- yar2 %>%
    autoplot() +
    ylab('y') +
    ggtitle('AR(2) time series')

# Arrange plots on grid
grid.arrange(p1, p2, nrow=1)

```

### HA 8.8

#### HA 8.8 (a)

Consider austa, the total international visitors to Australia (in millions) for the period 1980-2015.

**Use auto.arima() to find an appropriate ARIMA model. What model was selected? Check that the residuals look like white noise. Plot forecasts for the next 10 periods.**

First, we will plot the international visitors to Australia to get an idea for the data.

```{r}

# Load data
data(austa)

# Plot
austa %>%
    ggtsdisplay(main='Total international visitors to Australia (millions, 1980-2015)')

```

Now, we can use auto.arima to select a model automatically.

```{r}

# Fit auto.arima model and show results
fit <- auto.arima(austa)
summary(fit)

```

The model selected was ARIMA(0,1,1) with drift.

We need to check the residuals from the chosen model by plotting the ACF of the residuals and making sure they look like white noise.

```{r}

# Plot residuals
checkresiduals(fit)

```

The residuals are all within the bounds and therefor classify as white noise.

The next 10 time periods can be forecast.

```{r}

# Plot forecasts for the next ten periods
fit %>%
    forecast(h=10) %>%
    autoplot()

```

In summary, the auto.arima() function fit an ARIMA(0,1,1) model with drift. Using the checkresiduals() function, residuals appear as white noise, all lagged values are within the threshold limits, and the Ljung-Box p-value of 0.8905 exceeds 0.05. We can therefore conclude that this is an appropriate model to use.

#### HA 8.8 (b)

**Plot forecasts from an ARIMA(0,1,1) model with no drift and compare these to part a. Remove the MA term and plot again.**

First we can create forecasts from the ARIMA(0,1,1) model with no drift.

```{r}

# Model using ARIMA(0,1,1) with no drift
fit2 <- Arima(austa, order=c(0,1,1), include.drift=F)
summary(fit2)

# Plot
fit2 %>%
    forecast(h=10) %>%
    autoplot()

```

Compared to part A rather than having an increasing trend there is now a flat line projection.

Next we can remove the MA term and replot.

```{r}

# Remove the MA term, i.e., model using ARIMA(0,1,0) with no drift
fit3 <- Arima(austa, order=c(0,1,0), include.drift=F)
summary(fit3)

# Plot
fit3 %>%
    forecast(h=10) %>%
    autoplot()

```

As shown above, allowing drift in the model causes trend to be included in the forecast values, while point forecasts for the no-drift model are held constant. Removing the MA term generated similar point forecasts, but had a lower AICc than the ARIMA(0,1,1) model, indicating that the MA term may not be needed and that a simpler, random-walk model may be better.

#### HA 8.8 (c)

**Plot forecasts from an ARIMA(2,1,3) model with drift. Remove the constant and see what happens.**

First, plot forecasts from ARIMA(2,1,3) with drift.

```{r}

# Fit ARIMA(2,1,3) model with drift
fit4 <- Arima(austa, order=c(2,1,3), include.drift=T)
summary(fit4)

# Plot
fit4 %>%
    forecast(h=10) %>%
    autoplot()

```

Now remove the constant.

```{r}

# Fit ARIMA(2,1,3) model with drift, removing constant;
# generates an error - non-stationary series
# fit5 <- Arima(austa, order=c(2,1,3), include.drift=T, include.constant=F)

# Model using second-order differencing instead
fit5 <- Arima(austa, order=c(2,2,3), include.drift=T, include.constant=F)
summary(fit5)

# Plot
fit5 %>%
    forecast(h=10) %>%
    autoplot()

```

Removing the constant from the model elicits an error from Arima() indicating that the AR part of the model is non-stationary. Performing another round of differencing fixes the problem, i.e. using an ARIMA(2,2,3) model, but it yields a model with a higher AICc than the ARIMA(2,1,3) model with drift.

#### HA 8.8 (d)

**Plot forecasts from an ARIMA(0,0,1) model with a constant. Remove the MA term and plot again.**

First plot forecast from ARIMA(0,0,1) with a constant.

```{r}

# Model ARIMA(0,0,1) with constant
fit6 <- Arima(austa, order=c(0,0,1), include.constant=T)
summary(fit6)

# Plot forecasts
fit6 %>%
    forecast(h=10) %>%
    autoplot()

```

Now we can remove the MA term and plot again.

```{r}

# Remove the MA term, i.e., model ARIMA(0,0,0) with constant
fit7 <- Arima(austa, order=c(0,0,0), include.constant=T)
summary(fit7)

# Plot forecasts
fit7 %>%
    forecast(h=10) %>%
    autoplot()

```

One moving average with a constant yields what appears to be the mean of the series as the forecast mean. Removing the constant expands the 95% prediction interval from zero to just greater than the last recorded observation.

#### HA 8.8 (e)

**Plot forecasts from an ARIMA(0,2,1) model with no constant.**

```{r}

# Remove the MA term, i.e., model ARIMA(0,0,0) with constant
fit8 <- Arima(austa, order=c(0,2,1), include.constant=F)
summary(fit8)

# Plot forecasts
fit8 %>%
    forecast(h=10) %>%
    autoplot()

```

This looks remarkably similar to the model with the auto.arima() function selected, albeit with a wider prediction interval.
