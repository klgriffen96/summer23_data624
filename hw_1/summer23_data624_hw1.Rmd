---
title: 'Data 624: Predictive Analytics HW 1'
author: "Group 2: Alice Friedman, Kayleah Griffen, Josh Iden, Michael Ippolito"
date: "6/18/2023"
output:
  word_document: default
  html_document:
    df_print: paged
always_allow_html: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = TRUE)
```
## Introduction

This homework assignment includes problems from:

(1) Hyndman & Athanasopoulos. "Forecasting: Principles and Practice"
(2) Kuhn & Johnson. "Applied Predictive Modeling"

This accompanies readings from KJ 1,2 and 3 and HA 1,2,6,7 and 8.

## Week 1 Homework Solutions

### HA 2.1

Using the help function (`?`) to explore what the series `gold`, `woolyrnq` and `gas` represent, we get the following results in the description:

?gold
 "Daily morning gold prices in US dollars. 1 January 1985 – 31 March 1989"
 
?woolyrnq
"Quarterly production of woollen yarn in Australia: tonnes. Mar 1965 – Sep 1994"
 
?gas
Australian monthly gas production: 1956–1995.


#### a. Use autoplot() to plot each of these in separate plots.

```{r 2.1a, warning = FALSE, message=FALSE}
library(fpp2)
library(gridExtra)
library(tidyverse)



p1 = autoplot(gold) + ggtitle('Price of Gold: 1985-1989') + ylab('price') + xlab('')
p2 = autoplot(woolyrnq) + ggtitle('Quarterly Production of Woollen Yarn in Australia: 1965 - 1994') + ylab('tonnes') + xlab('')
p3 = autoplot(gas) + ggtitle('Australian monthly gas production: 1956-1995') + ylab('gas') + xlab('')

grid.arrange(p1, p2, p3, nrow=3)
```

#### b. What is the frequency of each series? 

```{r 2.1_frequency}

Gold <- frequency(gold)

Wool <- frequency(woolyrnq)

Gas <- frequency(gas)

Freqs <- data.frame(
  Series = c("Gold", "Wool", "Gas"),
  Frequency = c(Gold, Wool, Gas),
  Seasonality = c("Daily", "Quaterly", "Monthly")
  ) 
Freqs %>% print() 
```

Based on `frequency` and the help function you can tell the `gold` observations are taken daily, the `woolyrnq` is quarterly, and the `gas` is monthly.


#### c. Use which.max() to spot the outlier in the gold series. Which observation was it?

`which.max` determines the index of the maximum of the numeric vector. This occurs at day 770 in the gold series. We can check what date this is using `lubridate` package.

```{r, warning=FALSE, message = FALSE}
library(lubridate)

d <- ymd("1985/01/01") + which.max(gold)

paste0('day:', which.max(gold) ,  '  date:', d, '  price: $', gold[which.max(gold)])

```
The max price of gold occurred on February 10, 1987.

### HA 2.3

#### a. Download some monthly Australian retail data from the book website (https://otexts.com/fpp2/extrafiles/retail.xlsx). These represent retail sales in various categories for different Australian states, and are stored in a MS-Excel file.

Head of the downdloaded dataset is below.

```{r 2.3setup}
library(readxl)
retaildata <- readxl::read_excel("retail.xlsx", skip=1)
head(retaildata)
```

#### b. Select one of the time series as follows (but replace the column name with your own chosen column):

```{r 2.3b}
myts <- ts(retaildata[,"A3349335T"],
  frequency=12, start=c(1982,4))
```

#### c. Explore your chosen retail time series using the following functions: `autoplot()`, `ggseasonplot()`, `ggsubseriesplot()`, `gglagplot()`, `ggAcf()`

Below we plotted `autoplot()`, `ggseasonplot()`, `ggsubseriesplot()`, & ` ggAcf()` in a grid. The lag plot requires a lot of space so it is plotted seperately!

```{r 2.3}
ap <- autoplot(myts) +
  ggtitle("Autoplot: Monthly retail sales in various categories for different Australian states") +
  xlab("Year") +
  ylab("Retail Sales")

sp <- ggseasonplot(myts, year.labels=TRUE, year.labels.left=TRUE) +
  ylab("Retail Sales") +
  ggtitle("Seasonal plot: Monthly retail sales")

gsp <- ggsubseriesplot(myts) +
  ylab("Retail Sales") +
  ggtitle("Subseries Plot: Monthly retail sales")

myts_window <- window(myts, start = c(1982, 4))
win <- ggAcf(myts_window)


grid.arrange(ap, sp, gsp, win, ncol=2)
```

```{r}
lagged <- gglagplot(myts_window) + 
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1)) + 
  ggtitle("Lag Plot")
print(lagged)
```

#### Can you spot any seasonality, cyclicity and trend? What do you learn about the series?

* From the autoplot, we can see from the that there is a clear, increasing trend, as well as seasonality. 

* From the seasonal trend, it is easier to see that that the seasonality has an annual frequency which is additionally following a steady upward trend over time.

* The subseries plot again shows the trend clearly -- within each month there is a steady upward trend that appears to have roughly the same slope every month indicating that the trend is consistent across years.

* The window plot shows a clear trend (rather than white noise) which indicates that there is strong autocorrelation in the series.

* Finally, the lagged scatterplots show that the relationships are strongly positive across all lags, but dramatically positive at lag 12. This indicates a high annual correlation. We can see this in the ACF plot as well. We also notice that the correlation, while remaining positive, becomes less strongly positive as time goes by, and then peaks again every 12 months, again pointing to the 12 monthly frequency.


### HA 6.2

The `plastics` data set consists of the monthly sales (in thousands) of product A for a plastics manufacturer for five years.

#### a. Plot the time series of sales of product A. Can you identify seasonal fluctuations and/or a trend-cycle?

```{r 6.2a}
# ?plastics
# "Monthly sales of product A for a plastics manufacturer."

autoplot(plastics) +
  ggtitle("Monthly sales of product A for a plastics manufacturer") +
  xlab("Year") +
  ylab("Sales")
```

Based on the autoplot there does appear to be seasonality to the data, the plastic sales are highest May - October (peaking in August usually) and lower November - April (lowest in February). There is a consistent upward trend-cycle as well. 

#### b. Use a classical multiplicative decomposition to calculate the trend-cycle and seasonal indices.

We use the `decompose` function with `type="mutliplicative"` and then `autoplot` to decompose the series and then evalaute the results.

```{r 6.2b}
plastics %>% decompose(type="multiplicative") %>%
  autoplot() + xlab("Year") +
  ggtitle("Classical multiplicative decomposition
    of plasatic sales")
```

The trend cycle shows a strong seasonal component, with a yearly frequency, and has an increasing trend up until just past year 5 when it begins decreasing. There is some remainder as well. The remainder values below 1 indicate that there is some “leakage” of the trend-cycle component into the remainder component - the trend-cycle estimate has over-smoothed the drop in data.

#### c. Do the results support the graphical interpretation from part a?

Yes, the graphical interpretation from part A aligns with the multiplicative decomposition. The yearly seasonal pattern was noted in both a and b. The only part that was not captured well is the drop off in the trend after year 5. 

#### d. Compute and plot the seasonally adjusted data.

We can compute the seasonally adjusted fit by applying the `seasadj` function to the decomposed time series.

```{r 6.2d, warning=FALSE}
fit <- plastics %>%
  decompose(type="multiplicative") 

autoplot(plastics, series="Data") +
  autolayer(trendcycle(fit), series="Trend") +
  autolayer(seasadj(fit), series="Seasonally Adjusted") +
  xlab("Year") + ylab("Sales") +
  ggtitle("Monthly sales of product A for a plastics manufacturer") +
  scale_colour_manual(values=c("gray","blue","red"),
             breaks=c("Data","Seasonally Adjusted","Trend"))

```

#### e. Change one observation to be an outlier (e.g., add 500 to one observation), and recompute the seasonally adjusted data. What is the effect of the outlier?

Our approach is to create a funciton that allows us to quickly check different outliers and positions, then compare several outputs to the original seasonally adjusted plot.

```{r 6.2e, warning=FALSE}

#Function that creates plots based on the position (n) value (v) and whether it's an outlier or not (outlier)
outlier <- function(n, v, outlier){
  
  # if it's an ourlier, add the outlier and change the plot subtitle
  if(outlier==TRUE){
    plastics[n] <- v
    subtitle <- paste("Outlier is", v, "at month", n)
  } else {
    subtitle <- "Original data"
  }
  
  #fit to the decomp
  fit <- plastics %>% decompose(type="multiplicative") 
  
  #plot
  autoplot(plastics, series="Data") +
    autolayer(seasadj(fit), series="Seasonally Adjusted") +
    xlab("Year") + ylab("Sales") +
    labs(
      title = "Monthly sales of product A for a plastics manufacturer",
      subtitle = subtitle
      ) +
    scale_colour_manual(values=c("gray","blue"),
               breaks=c("Data","Seasonally Adjusted","Trend")) + 
    scale_y_continuous(expand = c(0, 0), limits = c(0, NA))

  
}

# plot the original and two variations in a grid
grid.arrange(
  outlier(1, 1, FALSE),
  outlier(59, 50, TRUE),
  outlier(30, 50, TRUE), 
  ncol=1)

```

The outlier in the middle causes the seasonally adjusted fit to hallucinate seasons that don't exist! At or near the end, there is no discernible effect.

#### f. Does it make any difference if the outlier is near the end rather than in the middle of the time series?

Yes -- the outlier in the middle has a much more pronounced affect than an outlier at the end.

If the outlier is at the end vs in the middle, there is more of an effect on the seasonally adjusted data. The estimate of the trend cycle is unavailable for the first and last few observations - without this there is also no estimate of the remainder component. Due to this, all of the outlier is passed on to the seasonally adjusted rather than having some of it put in the remainder or in the trend.

## Week 2 Homework Solutions

### HA 7.1 Consider the `pigs` series -- the number of pigs slaughtered in Victoria each month.

### 7.1 a Use the `ses` function to find the optimal levels of alpha and l-zero, and generate forecasts for the next 4 months.

```{r 7.1a}
fit_pigs <- pigs %>% ses(h=4)

fit_pigs %>% autoplot()

#summary(fit_pigs)
```
By callng `summary` on the results of the `ses` object, we can see that the optimal values of alpha and l-zero are 0.2971 and 77260.0561, respectively.

The first four predicted values are given by calling the `mean` operator on the `summary` object, as shown below. 

```{r 7.1aforecasts}
summary(fit_pigs)$mean
```

### 7.1 b Compute a 95% prediction interval for the first forecast using y-hat is plus or minus 1.96s where `s` is the standard deviation of the residuals. Compare your interval with the interval produced by R.

```{r 7.1b}
# margin of error
me <- 1.96*sd(fit_pigs$residuals)

# first forecast
f1 <- fit_pigs$mean[1]
```

The first forecast from the `ses` prediction on pigs is `r round(f1, 2)` with a 95% margin of error plus or minus `r round(me, 2)`. A comparison to results from R is shown below. The results are within 1/10 of 1% from the manually calculated values.

```{r 7.1b_table}
library(dplyr)

#Make a table to review results
labels_7.1b <- c("My Results", "R")

df_pigs <- data.frame(
  c(f1-me, f1+me), 
  c(fit_pigs$lower[5], fit_pigs$upper[5]))

colnames(df_pigs) <- labels_7.1b
row.names(df_pigs) <- c("Lower 95% Bound", "Upper 95% Bound")
df_pigs <- df_pigs %>% mutate(
  `% Diff` = round((`My Results` - R)/R*100,2)
)

df_pigs %>% t()
```


### HA 7.3

#### Modify your function from the previous exercise to return the sum of the squared errors rather than the forecast of the next observations. The use the `optim` function to find the optimal methods of alpha and l-0. Do you get the same values as the `ses()` function?

Note: In the previous exercise, we we asked to write our own function to implement simple exponential smoothing.

```{r 7.2, include=FALSE, eval=FALSE}
T <- length(pigs) %>% as.numeric()

my_smooth <- function(y, alpha, level) {
  
  T <- length(y) %>% as.numeric()
  
  my_fitted <- c()
  
  #set first value 
  my_fitted[1] <- level

    for (i in 2:(T+1)) {
    my_fitted[i] <- alpha * y[i-1] + (1 - alpha) * my_fitted[i-1]
  }
  
  #return last value
  return(my_fitted)
  }

alpha <- 0.291
level <- pigs[1]

my_smooth(pigs, alpha=alpha, level=level)[T+1]
my_smooth(pigs, alpha=alpha, level=level/2)[T+1]
my_smooth(pigs, alpha=1/T, level=level)[T+1]
my_smooth(pigs, alpha=1/T, level=level/2)[T+1]

ses(pigs, alpha=alpha)$mean[1]

```

The optim() function using the default method (Nelder and Mead) produced different values for α and $ℓ_0$ (0.5179980 and 0.1307012, respectively). Changing the method to BFGS (Broyden, Fletcher, Goldfarb and Shanno) made the results very close to the one produced by the ses() function (0.297142 and 77273.160331, respectively).

```{r}
# Function to implement SES, modified to return the sum of squared errors instead of the forecast of the next observation
my_ses_mod <- function(y, params) {

    # Split out parameters for optim() function    
    alpha <- params[1]
    level <- params[2]
    
    # Initialize vector to store forecast values
    fitted <- c()
    fitted[1] <- level
    
    # Set T to be length of time series
    T <- length(y)
    
    # Iterate over terms
    for (j in seq(2, T)) {
        fitted[j] <- alpha * (y[j - 1]) + (1 - alpha) * fitted[j - 1]
    }
    
    # Convert fitted values to time series
    fitted <- ts(fitted, frequency=12, start=c(1980, 1))
    
    # Calculate residuals
    residuals <- y - fitted
    
    # Calculate sum of squared errors
    sse=sum(residuals ** 2)

    # Return fitted values, residuals, and sum of squared errors
    # Return sse
    return(sse)

}

# Calculate SSE using modified custom function
fit2 <- my_ses_mod(pigs, c(0.2971, 77260.0561))

# Compare this SSE to SSE from original fit using sse()

ans7.1b <- data_frame(
  MyFit = c(fit2),
  sesFit = c(sum(fit_pigs$residuals ** 2))
)

ans7.1b <- ans7.1b %>% 
  mutate(
  `% Diff` = round((MyFit - sesFit)/sesFit*100 , 2)
)

print(ans7.1b)

# Optimize α and ℓ_0 using optim() function
fit3 <- optim(par=c(0, 0), fn=my_ses_mod, y=pigs)
fit3$par  # produces alpha=0.5179980, l_0=0.1307012

# Try optimize again using method='BFGS'
fit4 <- optim(par=c(0, 0), fn=my_ses_mod, y=pigs, method='BFGS')
fit4$par


```


## KJ 3.1
```{r 3.1}
library(mlbench)
data(Glass)
```

### 3.1a
Using visualizations, explore the predictor variables to understand their distributions as well as the relationships between predictors.

```{r 3.1a_pLots, message=FALSE, warning=FALSE}

# Gather
Glass_long <- Glass %>% select(-Type) %>% gather() 

# Hist plots
Glass_long %>% ggplot(aes(value)) + geom_histogram() + facet_wrap(~key, scales = "free")

# Box plots
Glass_long %>% ggplot(aes(y=value))+geom_boxplot()+facet_wrap(~key, scales = "free")
  
```

A colorized correlation plot is used to understand relationships between predictors.

```{r 3.1a_corrPLots}
library(corrplot)
library(caret)
# Filter for just quantitative vars
glassQuant <- Glass %>% select (-Type)

glasscorr <- cor(glassQuant)
# Corr chart
corrplot(cor(glasscorr), method="number", order = "AOE", type = "lower", diag = FALSE)

# Find correlations past cutoff
print(paste0("Candidate for removal due to high correlation: ", findCorrelation(glasscorr, cutoff=0.8, exact=T, verbose=T, names=T)))
```

There is some collinearity between `RI` and `Ca`. There is some correlation between a few of the other variables: `RI` and `Si`, `Mg` and `AI`, and `Ba` and `AI`.

The RI-Ca correlation is the only one to take particular note of, as it could impact the types of modeling sensitive to those effect (e.g. linear models). Using the heuristic algorithm proposed by K&J (and using the `findCorrelation()` function), calcium is a candidate for removal due to high correlation. 

All variables except Type are quantitative; Type is a categorical variable with 6 levels (1, 2, 3, 5, 6, and 7).

### 3.1b Do there appear to be any outliers? Are any predictors skewed?

As we observed from the histograms, all of the predictors are skewed to some degree, but the worst offenders are `Mg`, which appears to be bimodal, and `K`, `Ba`, and `Fe`, which appear to be right-skewed. We can confirm this by using the `skewness` function:

```{r 3.1b}
library(psych)
describe(Glass) %>% select(skew)
```

As far as outliers, let's take a look the boxplots above. There are a lot of outliers present (greater than 1.5X outside the interquartile range) in each predictor with the exception of `Mg`.

## KJ 3.2

### KJ 3.2 (a)

The soybean data can also be found at the UC Irvine Machine Learning Repository. Data were collected to predict disease in 683 soybeans. The 35 predictors are mostly categorical and include information on the environmental conditions (e.g., temperature, precipitation) and plant conditions (e.g., left spots, mold growth). The outcome labels consist of 19 distinct classes.

**Investigate the frequency distributions for the categorical predictors. Are any of the distributions degenerate in the ways discussed earlier in this chapter?**

Using the nearZeroVar() function, these variables appear to be degenerate as described in K&J as having near-zero variance:

* leaf.mild
* mycelium
* sclerotia

As shown in the summary output, each of these columns has a dominant class that far exceeds its next most populous class.

```{r}

# Load data
data(Soybean)
#str(Soybean)

# Generate frequency tables to look for degenerate distributions
multifunc2 <- function(x) {
    return(prop.table(table(x)))
}

# Degenerate vars: if both of these are true:
# 1. fraction of unique vals over the sample size < 10%
# 2. ratio of freq of the most prevalent to the freq of the second most prevalent is > 20
nzv <- nearZeroVar(Soybean)
#print(colnames(Soybean[,nzv]))

# Examine NZV columns
summary(Soybean[,nzv])

```

### KJ 3.2 (b)

**Roughly 18% of the data are missing. Are there particular predictors that are more likely to be missing? Is the pattern of missing data related to the classes?**

As shown in the md.pattern plot below, there are 562 (of 683) complete cases, or roughly 82% of the data. Of the 18% of the data that have over 15% missing values, most of the variables are related to plant pathology:

* hail
* sever
* seed.tmt
* germ
* leaf.mild
* lodging
* fruiting.bodies
* fruit.spots
* seed.discolor

We verified that the missing values weren't due to the absence of the specific pathology, as negative cases were also reported. So the values do appear to be missing as opposed to being intentionally or structurally absent.

```{r fig.width=6, fig.height=6}
col_sums = colSums(is.na(Soybean))

data.frame(variable = names(col_sums), missing = col_sums, row.names = NULL) |> 
  ggplot(aes(y=reorder(variable,missing), x=missing)) +
  geom_col() +
  ggtitle('Missing Values') + xlab('') + ylab('')

```

We see a bunch of predictors with more than 80 NAs, related to seed (`seed.tmt`,`seed.size`,`seed.discolor`,`seed`), leaf information (`leaf.size`,`leaf.shread`,`leaf.mild`,`leaf.marg`,`leaf.malf`,`leaf.halo`), fruit (`fruiting.bodies`,`fruit.spots`,`fruit.pods`), and a few others that are not of a clear category (`shriveling`,`sever`,`mold.growth`,`lodging`,`hail`,`germ`).

Checking to see how these NAs relate by class:

```{r}
Soybean |>
  group_by(class = Class) |>
  summarize(total_na = sum(is.na(across(-Class))),
            pct_na=sum(is.na(across(-Class)))
            / n()) |>
  arrange(desc(pct_na))
```

We see that five of the 19 classes are responsible for all of the NAs. This indicates to me a likelihood that certain measurements are either applicable or not available to these classes. This should be taken into consideration when considering how to deal with these missing values.

### KJ 3.2 (c)

**Develop a strategy for handling missing data, either by eliminating predictors or imputation.**

The degenerate values should be removed as they offer no predictive value.

Because NA values only appear in 5 of the 14 classes, missingness is potentially informative in predicting a class. Two ways to deal with this while retaining the predictive value of the missingness is to **1)** create an **indicator variable** that represents the presence or absence of missing values in a specific variable, or **2)** create a **missingness pattern variable** that indicates whether a specific combination of variables is missing or not.

It's also possible to apply pattern-based imputation using the `mice` or `amelia` packages in R.

```{r}
# Create copy of the original data set
sb2 <- Soybean

# Remove the observations in which 19 or more variables have missing values
most_complete <- rowSums(is.na(sb2)) < 19
sb2 <- sb2[most_complete,]

# Recalculate number of rows contain missing values
some_missing <- rowSums(is.na(sb2)) > 0
print(paste0("Proportion of observations with missing values after removing those with high proportion of missing variables: ", 
    round(nrow(sb2[some_missing,]) / nrow(sb2), 3)))
print(paste0("Proportion of observations retained from original data set: ",
    round(nrow(sb2) / nrow(Soybean), 3)))

```

An alternative approach would be to remove the three variables with degenerate classes:

* leaf.mild
* mycelium
* sclerotia

and to additionally remove those with a high proportion of data:

* hail
* sever
* seed.tmt
* germ
* leaf.mild (also degenerate)
* lodging
* fruiting.bodies
* fruit.spots
* seed.discolor

However, that still leaves 17.7% of observations with at least one missing value. So a combination of removing some observations along with the above variables would be more effective. The disadvantage of this is that 11 variables would be removed--a full 30% of all variables.

```{r}

# Create copy of original data set
sb3 <- Soybean

# Remove the observations in which 24 or more variables have missing values
most_complete <- rowSums(is.na(sb3)) < 24
sb3 <- sb3[most_complete,]

# Remove variables with degenerate classes or having high proportion of missing values (11 all together)
sb3 <- sb3 %>%
    select(-leaf.mild, -mycelium, -sclerotia, -hail, -sever, -seed.tmt, -germ, -lodging, -fruiting.bodies, -fruit.spots, -seed.discolor)

# Recalculate number of rows contain missing values
some_missing <- rowSums(is.na(sb3)) > 0
print(paste0("Proportion of observations with missing values after removing degenerate variables: ", 
    round(nrow(sb3[some_missing,]) / nrow(sb3), 3)))
print(paste0("Proportion of observations retained from original data set: ",
    round(nrow(sb3) / nrow(Soybean), 3)))

```

Another approach would be to impute the missing data. The MICE package (multivariate imputation by chained equations) is an appropriate way to do this. Even using MICE, the observations with over 24 variables missing would seem to remain problematic, so we'll remove those first before imputing. As shown below, we now have a dataset with no missing cases, and over 95% of the original observations were retained.

```{r warning=FALSE}

# Create copy of original data set
sb4 <- Soybean

# Remove the observations in which 24 or more variables have missing values
most_complete <- rowSums(is.na(sb4)) < 24
sb4 <- sb4[most_complete,]

# Recalculate number of rows contain missing values
some_missing <- rowSums(is.na(sb4)) > 0
print(paste0("Proportion of observations with missing values after removing degenerate variables: ", 
    round(nrow(sb4[some_missing,]) / nrow(sb4), 3)))
print(paste0("Proportion of observations retained from original data set: ",
    round(nrow(sb4) / nrow(Soybean), 3)))

# Impute missing data
imp <- mice(sb4, maxit=5, m=5, seed=777)

# Complete the data set using imputed values
sb4 <- complete(imp)

# Recalculate number of rows contain missing values
some_missing <- rowSums(is.na(sb4)) > 0
print(paste0("Proportion of observations with missing values after removing degenerate variables: ", 
    round(nrow(sb4[some_missing,]) / nrow(sb4), 3)))
print(paste0("Proportion of observations retained from original data set: ",
    round(nrow(sb4) / nrow(Soybean), 3)))

```


## Week 3 Homeowork Solutions

### Plot forecasts from an ARIMA(0,2,1) model with no constant.

This is done by adjusting the `include.constant` argument in `Arima`.

```{r}

# Remove the MA term, i.e., model ARIMA(0,0,0) with constant
fit8 <- Arima(austa, order=c(0,2,1), include.constant=F)
summary(fit8)

# Plot forecasts
fit8 %>%
    forecast(h=10) %>%
    autoplot()

```
